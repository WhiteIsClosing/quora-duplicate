{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathrooms\n",
      "bedrooms\n",
      "building_id\n",
      "created\n",
      "description\n",
      "display_address\n",
      "features\n",
      "interest_level\n",
      "latitude\n",
      "listing_id\n",
      "longitude\n",
      "manager_id\n",
      "photos\n",
      "price\n",
      "street_address\n"
     ]
    }
   ],
   "source": [
    "with open('./train.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "raw_train = pd.DataFrame(data)\n",
    "\n",
    "with open('./test.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "raw_test = pd.DataFrame(data)\n",
    "\n",
    "raw_test = pd.DataFrame(data)\n",
    "\n",
    "for item in raw_train.columns:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id is used for identifying the listing in the competition. For simplicity, we reindex the data by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_id = raw_train.index\n",
    "raw_train = raw_train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a popular model xgboost. We will start with only the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']\n",
    "X = raw_train[col]\n",
    "y = raw_train['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding\n",
    "\n",
    "To be able to evaluate our model, we use the package model_selection to split the indices into:\n",
    "    - Training data (used to train the models)\n",
    "    - Validation data (used to evaluate the models. We avoid using the term 'test data' to differentiate this from \n",
    "    the test data from Kaggle)\n",
    "\n",
    "For the later usage in stacking, we also split the training data in 2 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=3)\n",
    "folds = skf.split(X, y)\n",
    "_, fold1 = folds.next()\n",
    "_, fold2 = folds.next()\n",
    "_, validation_idx = folds.next()\n",
    "\n",
    "train_idx = np.concatenate([fold1, fold2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ..., 16867 16904 16919]\n",
      "[16331 16332 16333 ..., 33063 33066 33070]\n",
      "[32851 32853 32854 ..., 49349 49350 49351]\n"
     ]
    }
   ],
   "source": [
    "print fold1\n",
    "print fold2\n",
    "print validation_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we initialize the paramters for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['eta'] = 0.02\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric'] = 'mlogloss'\n",
    "param['min_child_weight'] = 3\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['seed'] = 321\n",
    "num_rounds = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is: 0.656 \n",
      "Time elapsed: 33.13 seconds \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train = np.array(X.iloc[train_idx])\n",
    "y_train = np.array(y.iloc[train_idx])\n",
    "xgtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "clf = xgb.train(param, xgtrain, num_rounds)\n",
    "\n",
    "X_validation = np.array(X.iloc[validation_idx])\n",
    "y_validation = np.array(y.iloc[validation_idx])\n",
    "xgvalidation = xgb.DMatrix(X_validation)\n",
    "y_prob = clf.predict(xgvalidation)\n",
    "\n",
    "print 'The log loss is: %.3f ' % sklearn.metrics.log_loss(y_validation, y_prob)\n",
    "print 'Time elapsed: %.2f seconds ' % (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32902, 5)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Text Data\n",
    "\n",
    "## Clean the text\n",
    "The code below cleans the \"description\" column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.encode('ascii', errors='replace')\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('[^\\w\\s]', ' ', sentence) # removes punctuations\n",
    "    sentence = re.sub('_', ' ', sentence) # removes punctuations\n",
    "    sentence = re.sub('\\d+', ' ', sentence) # removes digits\n",
    "    cleaned = ' '.join([w for w in sentence.split() if not w in stop]) # remove stopwords\n",
    "    cleaned = ' '.join([w for w, pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos == 'JJ')])\n",
    "    # selecting only nounds and adjectives\n",
    "    \n",
    "    cleaned = ' '.join([w for w in cleaned.split() if not len(w) <= 2])\n",
    "    # remove single lettered words and digits\n",
    "    \n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 293.17 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "raw_train['cleaned_text'] = raw_train['description'].apply(lambda x: clean_text(x))\n",
    "print 'Time elapsed: %.2f ' % (time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "To take the text data into account, we need to somehow encoding the categorical data features into \n",
    "numerical features. Dummification (or one-hot encoding) is a popular way for general categorical features.\n",
    "For text data, we would use a much more efficient method: tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "n_features = 500\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
    "text_train = np.array(raw_train.loc[train_idx, 'cleaned_text'])\n",
    "tf = tf_vectorizer.fit_transform(text_train)\n",
    "TF = TfidfTransformer()\n",
    "tf_idf_train = TF.fit_transform(tf)\n",
    "tf_idf_train = tf_idf_train.toarray()\n",
    "X_train_des = np.concatenate([X_train, tf_idf_train], axis=1)\n",
    "\n",
    "\n",
    "text_validation = np.array(raw_train.loc[validation_idx, 'cleaned_text'].replace(np.nan, ''))\n",
    "tf = tf_vectorizer.transform(text_validation)\n",
    "tf_idf_validation = TF.transform(tf)\n",
    "tf_idf_validation = tf_idf_validation.toarray()\n",
    "X_validation_des = np.concatenate([X_validation, tf_idf_validation], axis=1)\n",
    "\n",
    "text_fold1 = np.array(raw_train.loc[fold1, 'cleaned_text'].replace(np.nan, ''))\n",
    "text_fold2 = np.array(raw_train.loc[fold2, 'cleaned_text'].replace(np.nan, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16451,)\n",
      "(16451,)\n",
      "(16450,)\n"
     ]
    }
   ],
   "source": [
    "print text_fold1.shape\n",
    "print text_fold2.shape\n",
    "print text_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train the model with both the numerical feature we used before, and the new text data.\n",
    "We see that the performance is improved with no surprise -- we took more feature into account.\n",
    "However, the improvement takes a lot of time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_ = time.time()\n",
    "xgtrain_des = xgb.DMatrix(X_train_des, label=y_train)\n",
    "clf = xgb.train(param, xgtrain_des, num_rounds)\n",
    "xgvalidation_des = xgb.DMatrix(X_validation_des)\n",
    "y_prob_des = clf.predict(xgvalidation_des)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking models with resampling\n",
    "\n",
    "In the demo above we saw a drawback of the tree-based models: inefficiency when dealing with large cardinality of \n",
    "categorical feature.\n",
    "\n",
    "One thing we can do is to train some simpler models first on the text data, and then stack back with the numerical predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sparse matrix.\n",
    "\n",
    "The text data contains a lot of zeros, one simple way to gain efficiency is to use the sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = tf_vectorizer.transform(text_train) # fold1 & 2\n",
    "tf_idf_train = TF.transform(tf)\n",
    "\n",
    "tf = tf_vectorizer.transform(text_fold1)\n",
    "tf_idf_fold1 = TF.transform(tf)\n",
    "\n",
    "tf = tf_vectorizer.transform(text_fold2)\n",
    "tf_idf_fold2 = TF.transform(tf)\n",
    "\n",
    "tf = tf_vectorizer.transform(text_validation)\n",
    "tf_idf_validation = TF.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the new feature\n",
    "\n",
    "The function below create the new predictors with 2 folds we described in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_fold1 = np.array(y.iloc[fold1])\n",
    "y_fold2 = np.array(y.iloc[fold2])\n",
    "\n",
    "def get_2fold_stack(model):\n",
    "    model.fit(tf_idf_fold1, y_fold1)\n",
    "    new_fold2 = model.predict_proba(tf_idf_fold2)[:, :2]\n",
    "    v1 = model.predict_proba(tf_idf_validation)[:, :2]  ### There is model\n",
    "\n",
    "    model.fit(tf_idf_fold2, y_fold2)\n",
    "    new_fold1 = model.predict_proba(tf_idf_fold1)[:, :2]\n",
    "    v2 = model.predict_proba(tf_idf_validation)[:, :2] ### The model here is different\n",
    "    \n",
    "    return np.concatenate([new_fold1, new_fold2], axis=0), (v1+v2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we add the predictor created by logistic regression. The performance is actualy much better than \n",
    "training with only numerical features, even though doesn't require as much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16450, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is: 0.641\n",
      "Time elapsed: 26.99\n"
     ]
    }
   ],
   "source": [
    "start_ = time.time()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression()\n",
    "\n",
    "f_logit_train, f_logit_validation = get_2fold_stack(logit) \n",
    "# f_logit_train: prediction from fold1& 2 concat.\n",
    "# f_logit_validation: prediction from validation\n",
    "\n",
    "X_train_stack = np.concatenate([X_train, f_logit_train], axis=1) # X_train: numerical features from fold1 & 2\n",
    "xgtrain_stack = xgb.DMatrix(X_train_stack, label=y_train)\n",
    "X_validation_stack = np.concatenate([X_validation, f_logit_validation], axis=1)\n",
    "xgvalidation_stack = xgb.DMatrix(X_validation_stack)\n",
    "\n",
    "clf = xgb.train(param, xgtrain_stack, num_rounds)\n",
    "y_prob_stack = clf.predict(xgvalidation_stack)\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob_stack)\n",
    "print 'Time elapsed: %.2f' % (time.time() - start_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_ = time.time()\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "logit1000 = LogisticRegression(C=1000)\n",
    "f_logit1000_train, f_logit1000_validation = get_2fold_stack(logit1000)\n",
    "\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "f_knn10_train, f_knn10_validation = get_2fold_stack(knn10)\n",
    "\n",
    "knn100 = KNeighborsClassifier(n_neighbors=100)\n",
    "f_knn100_train, f_knn100_validation = get_2fold_stack(knn100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/Library/Python/2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "topic10 = LatentDirichletAllocation(n_topics=10)\n",
    "f_topic10_train = topic10.fit_transform(tf_idf_train)\n",
    "f_topic10_validation = topic10.fit_transform(tf_idf_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.1370008],\n",
       "       ..., \n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

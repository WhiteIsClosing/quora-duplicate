{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import operator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, subplot, specgram, imshow, savefig\n",
    "from sklearn.metrics import log_loss, make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "RS = 12357\n",
    "ROUNDS = 315\n",
    "\n",
    "print(\"Started\")\n",
    "np.random.seed(RS)\n",
    "input_folder = '../kaggle-quora/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_xgb(X, y, params):\n",
    "    print(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n",
    "    x, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n",
    "\n",
    "    xg_train = xgb.DMatrix(x, label=y_train)\n",
    "    xg_val = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    watchlist  = [(xg_train,'train'), (xg_val,'eval')]\n",
    "    return xgb.train(params, xg_train, ROUNDS, watchlist)\n",
    "\n",
    "def predict_xgb(clr, X_test):\n",
    "    return clr.predict(xgb.DMatrix(X_test))\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    i = 0\n",
    "    for feat in features:\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "        i = i + 1\n",
    "    outfile.close()\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "    x['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x[word + '_both'] = x['q1_' + word] * x['q2_' + word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    return 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "def word_shares(row):\n",
    "    q1_list = str(row['question1']).lower().split()\n",
    "    q1 = set(q1_list)\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    q2_list = str(row['question2']).lower().split()\n",
    "    q2 = set(q2_list)\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "    q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "    total_weights = q1_weights + q1_weights\n",
    "\n",
    "    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "    if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "        R2gram = 0\n",
    "    else:\n",
    "        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "    return '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: X_train: (404290, 6), X_test: (2345796, 3)\n",
      "Features processing, be patient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 50s, sys: 5.97 s, total: 4min 56s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.11\n",
    "params['max_depth'] = 5\n",
    "params['silent'] = 1\n",
    "params['seed'] = RS\n",
    "\n",
    "df_train = pd.read_csv(input_folder + 'train.csv')\n",
    "df_test  = pd.read_csv(input_folder + 'test.csv')\n",
    "\n",
    "print(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))\n",
    "print(\"Features processing, be patient...\")\n",
    "\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + \\\n",
    "                     df_train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df['word_shares'] = df.apply(word_shares, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 1.58 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n",
    "\n",
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']\n",
    "\n",
    "x['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n",
    "\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n",
    "\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n",
    "\n",
    "x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n",
    "\n",
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "x['duplicated'] = df.duplicated(['question1','question2']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Magic Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 992 ms, sys: 208 ms, total: 1.2 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_combine = pd.read_csv('../kaggle-quora/features/train_magic.csv', header=0)\n",
    "test_combine = pd.read_csv('../kaggle-quora/features/test_magic.csv', header=0)\n",
    "df_combine = pd.concat([train_combine, test_combine])\n",
    "x['q1_freq'] = df_combine['q1_freq']\n",
    "x['q2_freq'] = df_combine['q2_freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count',\n",
      "       'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine',\n",
      "       'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len',\n",
      "       'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1',\n",
      "       'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2',\n",
      "       'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word',\n",
      "       'exactly_same', 'duplicated', 'q1_freq', 'q2_freq', 'q1_how', 'q2_how',\n",
      "       'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which',\n",
      "       'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where',\n",
      "       'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why',\n",
      "       'why_both'],\n",
      "      dtype='object')\n",
      "         word_match  word_match_2root  tfidf_word_match  shared_count  \\\n",
      "count  2.744371e+06      2.744371e+06      2.750086e+06  2.750086e+06   \n",
      "mean   1.542752e-01      3.228801e-01      2.016003e-01  1.605620e+00   \n",
      "std    1.375446e-01      2.236598e-01      2.011869e-01  1.548127e+00   \n",
      "min    0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
      "50%    1.355803e-01      3.682123e-01      1.666667e-01  1.000000e+00   \n",
      "75%    2.456540e-01      4.956349e-01      2.857143e-01  2.000000e+00   \n",
      "max    5.000000e-01      7.071068e-01      1.000000e+00  3.200000e+01   \n",
      "\n",
      "       stops1_ratio  stops2_ratio  shared_2gram        cosine  words_hamming  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.739236e+06   2.750086e+06   \n",
      "mean   9.467299e-01  9.507897e-01  7.278536e-02  2.967060e-01   1.303409e-01   \n",
      "std    5.062331e-01  5.083392e-01  9.986519e-02  2.625285e-01   1.996097e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
      "25%    6.250000e-01  6.250000e-01  0.000000e+00  0.000000e+00   0.000000e+00   \n",
      "50%    8.333333e-01  8.571429e-01  3.333333e-02  2.673381e-01   2.702703e-02   \n",
      "75%    1.200000e+00  1.200000e+00  1.111111e-01  4.727389e-01   1.818182e-01   \n",
      "max    1.000000e+01  9.000000e+00  5.000000e-01  1.000000e+00   1.000000e+00   \n",
      "\n",
      "       diff_stops_r      ...           who_both      q1_where      q2_where  \\\n",
      "count  2.750086e+06      ...       2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean  -4.059835e-03      ...       1.067967e-02  2.500795e-02  2.491122e-02   \n",
      "std    6.107030e-01      ...       1.027892e-01  1.561491e-01  1.558546e-01   \n",
      "min   -8.000000e+00      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%   -3.214286e-01      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    3.000000e-01      ...       0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    8.400000e+00      ...       1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "         where_both       q1_when       q2_when     when_both        q1_why  \\\n",
      "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
      "mean   7.281590e-03  3.273570e-02  3.223208e-02  7.694668e-03  9.631372e-02   \n",
      "std    8.502101e-02  1.779441e-01  1.766159e-01  8.738114e-02  2.950210e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "             q2_why      why_both  \n",
      "count  2.750086e+06  2.750086e+06  \n",
      "mean   9.600136e-02  3.267607e-02  \n",
      "std    2.945932e-01  1.777874e-01  \n",
      "min    0.000000e+00  0.000000e+00  \n",
      "25%    0.000000e+00  0.000000e+00  \n",
      "50%    0.000000e+00  0.000000e+00  \n",
      "75%    0.000000e+00  0.000000e+00  \n",
      "max    1.000000e+00  1.000000e+00  \n",
      "\n",
      "[8 rows x 50 columns]\n",
      "CPU times: user 37.2 s, sys: 4.27 s, total: 41.5 s\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')\n",
    "\n",
    "print(x.columns)\n",
    "print(x.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count', 'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine', 'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len', 'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1', 'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2', 'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word', 'exactly_same', 'duplicated', 'q1_freq', 'q2_freq', 'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both']\n",
      "Oversampling started for proportion: 0.369197853026293\n",
      "Oversampling done, new proportion: 0.19124366100096607\n",
      "CPU times: user 735 ms, sys: 893 ms, total: 1.63 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "feature_names = list(x.columns.values)\n",
    "create_feature_map(feature_names)\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "\n",
    "x_train = x[:df_train.shape[0]]\n",
    "x_test  = x[df_train.shape[0]:]\n",
    "y_train = df_train['is_duplicate'].values\n",
    "del x, df_train\n",
    "\n",
    "if 1: # Now we oversample the negative class - on your own risk of overfitting!\n",
    "    pos_train = x_train[y_train == 1]\n",
    "    neg_train = x_train[y_train == 0]\n",
    "\n",
    "    print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "    x_train = pd.concat([pos_train, neg_train])\n",
    "    y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "    del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> \n",
      "\n",
      "(780486, 50)\n",
      "780486\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train), '\\n')\n",
    "print(x_train.shape)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_match</th>\n",
       "      <th>word_match_2root</th>\n",
       "      <th>tfidf_word_match</th>\n",
       "      <th>shared_count</th>\n",
       "      <th>stops1_ratio</th>\n",
       "      <th>stops2_ratio</th>\n",
       "      <th>shared_2gram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>words_hamming</th>\n",
       "      <th>diff_stops_r</th>\n",
       "      <th>...</th>\n",
       "      <th>who_both</th>\n",
       "      <th>q1_where</th>\n",
       "      <th>q2_where</th>\n",
       "      <th>where_both</th>\n",
       "      <th>q1_when</th>\n",
       "      <th>q2_when</th>\n",
       "      <th>when_both</th>\n",
       "      <th>q1_why</th>\n",
       "      <th>q2_why</th>\n",
       "      <th>why_both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176680</td>\n",
       "      <td>0.420334</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.287964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212054</td>\n",
       "      <td>0.460493</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.464290</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.191159</td>\n",
       "      <td>0.437218</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.464872</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_match  word_match_2root  tfidf_word_match  shared_count  stops1_ratio  \\\n",
       "0    0.176680          0.420334          0.153846           2.0      0.833333   \n",
       "1    0.212054          0.460493          0.333333           3.0      1.000000   \n",
       "2    0.191159          0.437218          0.285714           2.0      1.000000   \n",
       "3    0.000000          0.000000          0.000000           0.0      1.000000   \n",
       "4    0.500000          0.707107          0.666667           2.0      0.333333   \n",
       "\n",
       "   stops2_ratio  shared_2gram    cosine  words_hamming  diff_stops_r  \\\n",
       "0      0.444444      0.043478  0.287964       0.000000      0.388889   \n",
       "1      0.400000      0.105263  0.464290       0.142857      0.600000   \n",
       "2      1.000000      0.055556  0.464872       0.071429      0.000000   \n",
       "3      0.500000      0.000000  0.000000       0.000000      0.500000   \n",
       "4      2.000000      0.125000  1.000000       0.166667     -1.666667   \n",
       "\n",
       "     ...     who_both  q1_where  q2_where  where_both  q1_when  q2_when  \\\n",
       "0    ...            0         0         0           0        0        0   \n",
       "1    ...            0         0         0           0        0        0   \n",
       "2    ...            0         0         0           0        0        0   \n",
       "3    ...            0         0         0           0        0        0   \n",
       "4    ...            0         0         0           0        0        0   \n",
       "\n",
       "   when_both  q1_why  q2_why  why_both  \n",
       "0          0       0       1         0  \n",
       "1          0       0       0         0  \n",
       "2          0       0       0         0  \n",
       "3          0       0       0         0  \n",
       "4          0       0       0         0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_match          0\n",
       "word_match_2root    0\n",
       "tfidf_word_match    0\n",
       "shared_count        0\n",
       "stops1_ratio        0\n",
       "stops2_ratio        0\n",
       "shared_2gram        0\n",
       "cosine              0\n",
       "words_hamming       0\n",
       "diff_stops_r        0\n",
       "len_q1              0\n",
       "len_q2              0\n",
       "diff_len            0\n",
       "caps_count_q1       0\n",
       "caps_count_q2       0\n",
       "diff_caps           0\n",
       "len_char_q1         0\n",
       "len_char_q2         0\n",
       "diff_len_char       0\n",
       "len_word_q1         0\n",
       "len_word_q2         0\n",
       "diff_len_word       0\n",
       "avg_world_len1      0\n",
       "avg_world_len2      0\n",
       "diff_avg_word       0\n",
       "exactly_same        0\n",
       "duplicated          0\n",
       "q1_freq             0\n",
       "q2_freq             0\n",
       "q1_how              0\n",
       "q2_how              0\n",
       "how_both            0\n",
       "q1_what             0\n",
       "q2_what             0\n",
       "what_both           0\n",
       "q1_which            0\n",
       "q2_which            0\n",
       "which_both          0\n",
       "q1_who              0\n",
       "q2_who              0\n",
       "who_both            0\n",
       "q1_where            0\n",
       "q2_where            0\n",
       "where_both          0\n",
       "q1_when             0\n",
       "q2_when             0\n",
       "when_both           0\n",
       "q1_why              0\n",
       "q2_why              0\n",
       "why_both            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.fillna(x_train.mean())\n",
    "x_test = x_test.fillna(x_test.mean())\n",
    "np.sum(x_test.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 56s, sys: 6.99 s, total: 2min 3s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pd.to_pickle(x_train, \"./train_jaccard_magic_features.pkl\")\n",
    "pd.to_pickle(x_test, \"./test_jaccard_magic_features.pkl\")\n",
    "\n",
    "x_train.to_csv('./train_jaccard_magic_features.csv')\n",
    "x_test.to_csv('./test_jaccard_magic_features.csv')\n",
    "\n",
    "pd.DataFrame(data=y_train).to_csv('./y_train_oversampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for 3 fold stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=3, random_state=None)\n",
    "folds = skf.split(x_train, y_train)\n",
    "_, fold1_index = next(folds)\n",
    "_, fold2_index = next(folds)\n",
    "_, validation_index = next(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=fold1_index).to_csv('./fold1_index.csv', index=False)\n",
    "pd.DataFrame(data=fold2_index).to_csv('./fold2_index.csv', index=False)\n",
    "pd.DataFrame(data=validation_index).to_csv('./validation_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260163,)\n",
      "(260162,)\n",
      "(260161,)\n"
     ]
    }
   ],
   "source": [
    "print(fold1_index.shape)\n",
    "print(fold2_index.shape)\n",
    "print(validation_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fold1 = x_train.iloc[fold1_index, :]\n",
    "train_fold2 = x_train.iloc[fold2_index, :]\n",
    "validation_fold = x_train.iloc[validation_index, :]\n",
    "\n",
    "y_train_fold1 = pd.DataFrame(data=y_train).iloc[fold1_index, :]\n",
    "y_train_fold2 = pd.DataFrame(data=y_train).iloc[fold2_index, :]\n",
    "y_validation_fold = pd.DataFrame(data=y_train).iloc[validation_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "### Use this cell if loading data ###\n",
    "#####################################\n",
    "\n",
    "fold1_index = pd.read_csv('./index/fold1_index.csv')\n",
    "fold2_index = pd.read_csv('./index/fold2_index.csv')\n",
    "validation_index = pd.read_csv('./index/validation_index.csv')\n",
    "\n",
    "x_train = pd.read_csv('../kaggle-quora/features/train_jaccard_magic_features.csv')\n",
    "y_train = pd.read_csv('../kaggle-quora/data/y_train_oversampled.csv')\n",
    "\n",
    "train_fold1 = x_train.loc[fold1_index.values.flatten(), :]\n",
    "train_fold2 = x_train.loc[fold2_index.values.flatten(), :]\n",
    "validation_fold = x_train.loc[validation_index.values.flatten(), :]\n",
    "\n",
    "y_train_fold1 = y_train.loc[fold1_index.values.flatten(), :]\n",
    "y_train_fold2 = y_train.loc[fold2_index.values.flatten(), :]\n",
    "y_validation_fold = y_train.loc[validation_index.values.flatten(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    210408\n",
      "1.0     49755\n",
      "Name: 0, dtype: int64\n",
      "0.0    210408\n",
      "1.0     49754\n",
      "Name: 0, dtype: int64\n",
      "0.0    210407\n",
      "1.0     49754\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train_fold1['0'].value_counts())\n",
    "print(y_train_fold2['0'].value_counts())\n",
    "print(y_validation_fold['0'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_2fold_stack(model):\n",
    "    xg_train_fold1 = xgb.DMatrix(train_fold1, label=y_train_fold1)\n",
    "    xg_train_fold2 = xgb.DMatrix(train_fold2, label=y_train_fold2)\n",
    "    xg_val = xgb.DMatrix(validation_fold, label=y_validation_fold)\n",
    "    \n",
    "    watchlist_fold1  = [(xg_train_fold1, 'train'), (xg_val, 'eval')]\n",
    "    watchlist_fold2  = [(xg_train_fold2, 'train'), (xg_val, 'eval')]\n",
    "    \n",
    "    clf1 = model.train(params, xg_train_fold1, ROUNDS, watchlist_fold1)\n",
    "    new_fold2 = clf1.predict(xg_train_fold2)\n",
    "    v1 = clf1.predict(xg_val)\n",
    "\n",
    "    clf2 = model.train(params, xg_train_fold2, ROUNDS, watchlist_fold2)\n",
    "    new_fold1 = clf2.predict(xg_train_fold1)\n",
    "    v2 = clf2.predict(xg_val)\n",
    "    \n",
    "    return np.concatenate([new_fold1, new_fold2], axis=0), (v1+v2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.629312\teval-logloss:0.629403\n",
      "[1]\ttrain-logloss:0.57803\teval-logloss:0.578199\n",
      "[2]\ttrain-logloss:0.535662\teval-logloss:0.535892\n",
      "[3]\ttrain-logloss:0.500716\teval-logloss:0.501025\n",
      "[4]\ttrain-logloss:0.470908\teval-logloss:0.471321\n",
      "[5]\ttrain-logloss:0.446152\teval-logloss:0.446638\n",
      "[6]\ttrain-logloss:0.424759\teval-logloss:0.425335\n",
      "[7]\ttrain-logloss:0.40648\teval-logloss:0.407123\n",
      "[8]\ttrain-logloss:0.390748\teval-logloss:0.39145\n",
      "[9]\ttrain-logloss:0.376592\teval-logloss:0.377335\n",
      "[10]\ttrain-logloss:0.364415\teval-logloss:0.365211\n",
      "[11]\ttrain-logloss:0.353374\teval-logloss:0.354224\n",
      "[12]\ttrain-logloss:0.344412\teval-logloss:0.345308\n",
      "[13]\ttrain-logloss:0.336216\teval-logloss:0.337178\n",
      "[14]\ttrain-logloss:0.329425\teval-logloss:0.330433\n",
      "[15]\ttrain-logloss:0.323138\teval-logloss:0.324205\n",
      "[16]\ttrain-logloss:0.317568\teval-logloss:0.318683\n",
      "[17]\ttrain-logloss:0.312817\teval-logloss:0.313977\n",
      "[18]\ttrain-logloss:0.30797\teval-logloss:0.309169\n",
      "[19]\ttrain-logloss:0.303414\teval-logloss:0.304662\n",
      "[20]\ttrain-logloss:0.299915\teval-logloss:0.301207\n",
      "[21]\ttrain-logloss:0.296297\teval-logloss:0.297589\n",
      "[22]\ttrain-logloss:0.293226\teval-logloss:0.294562\n",
      "[23]\ttrain-logloss:0.291006\teval-logloss:0.292394\n",
      "[24]\ttrain-logloss:0.28876\teval-logloss:0.290192\n",
      "[25]\ttrain-logloss:0.286885\teval-logloss:0.288361\n",
      "[26]\ttrain-logloss:0.284944\teval-logloss:0.286492\n",
      "[27]\ttrain-logloss:0.283337\teval-logloss:0.284937\n",
      "[28]\ttrain-logloss:0.282021\teval-logloss:0.283667\n",
      "[29]\ttrain-logloss:0.280869\teval-logloss:0.282559\n",
      "[30]\ttrain-logloss:0.279357\teval-logloss:0.281074\n",
      "[31]\ttrain-logloss:0.278219\teval-logloss:0.279975\n",
      "[32]\ttrain-logloss:0.277101\teval-logloss:0.278913\n",
      "[33]\ttrain-logloss:0.276337\teval-logloss:0.278182\n",
      "[34]\ttrain-logloss:0.275461\teval-logloss:0.277331\n",
      "[35]\ttrain-logloss:0.274512\teval-logloss:0.276399\n",
      "[36]\ttrain-logloss:0.273885\teval-logloss:0.275821\n",
      "[37]\ttrain-logloss:0.273346\teval-logloss:0.275315\n",
      "[38]\ttrain-logloss:0.272776\teval-logloss:0.27475\n",
      "[39]\ttrain-logloss:0.27203\teval-logloss:0.274026\n",
      "[40]\ttrain-logloss:0.271381\teval-logloss:0.273419\n",
      "[41]\ttrain-logloss:0.27091\teval-logloss:0.272973\n",
      "[42]\ttrain-logloss:0.270512\teval-logloss:0.272617\n",
      "[43]\ttrain-logloss:0.270109\teval-logloss:0.27224\n",
      "[44]\ttrain-logloss:0.269795\teval-logloss:0.271961\n",
      "[45]\ttrain-logloss:0.269481\teval-logloss:0.271659\n",
      "[46]\ttrain-logloss:0.269224\teval-logloss:0.271446\n",
      "[47]\ttrain-logloss:0.268879\teval-logloss:0.27115\n",
      "[48]\ttrain-logloss:0.268524\teval-logloss:0.270849\n",
      "[49]\ttrain-logloss:0.268264\teval-logloss:0.270611\n",
      "[50]\ttrain-logloss:0.267795\teval-logloss:0.27018\n",
      "[51]\ttrain-logloss:0.267322\teval-logloss:0.269745\n",
      "[52]\ttrain-logloss:0.266984\teval-logloss:0.269457\n",
      "[53]\ttrain-logloss:0.266741\teval-logloss:0.269255\n",
      "[54]\ttrain-logloss:0.266481\teval-logloss:0.269009\n",
      "[55]\ttrain-logloss:0.26624\teval-logloss:0.268817\n",
      "[56]\ttrain-logloss:0.265886\teval-logloss:0.268502\n",
      "[57]\ttrain-logloss:0.265757\teval-logloss:0.2684\n",
      "[58]\ttrain-logloss:0.265472\teval-logloss:0.268173\n",
      "[59]\ttrain-logloss:0.265052\teval-logloss:0.267791\n",
      "[60]\ttrain-logloss:0.264719\teval-logloss:0.267503\n",
      "[61]\ttrain-logloss:0.264409\teval-logloss:0.267254\n",
      "[62]\ttrain-logloss:0.264223\teval-logloss:0.267117\n",
      "[63]\ttrain-logloss:0.264128\teval-logloss:0.267042\n",
      "[64]\ttrain-logloss:0.263845\teval-logloss:0.266808\n",
      "[65]\ttrain-logloss:0.263587\teval-logloss:0.266598\n",
      "[66]\ttrain-logloss:0.263364\teval-logloss:0.266413\n",
      "[67]\ttrain-logloss:0.262855\teval-logloss:0.265939\n",
      "[68]\ttrain-logloss:0.262686\teval-logloss:0.265805\n",
      "[69]\ttrain-logloss:0.262573\teval-logloss:0.26572\n",
      "[70]\ttrain-logloss:0.262196\teval-logloss:0.265393\n",
      "[71]\ttrain-logloss:0.261878\teval-logloss:0.265104\n",
      "[72]\ttrain-logloss:0.261656\teval-logloss:0.264931\n",
      "[73]\ttrain-logloss:0.261506\teval-logloss:0.264809\n",
      "[74]\ttrain-logloss:0.261332\teval-logloss:0.26469\n",
      "[75]\ttrain-logloss:0.261195\teval-logloss:0.264588\n",
      "[76]\ttrain-logloss:0.260837\teval-logloss:0.264292\n",
      "[77]\ttrain-logloss:0.260671\teval-logloss:0.264143\n",
      "[78]\ttrain-logloss:0.26043\teval-logloss:0.263945\n",
      "[79]\ttrain-logloss:0.260277\teval-logloss:0.263827\n",
      "[80]\ttrain-logloss:0.260084\teval-logloss:0.263667\n",
      "[81]\ttrain-logloss:0.259963\teval-logloss:0.263589\n",
      "[82]\ttrain-logloss:0.259738\teval-logloss:0.263423\n",
      "[83]\ttrain-logloss:0.259497\teval-logloss:0.263208\n",
      "[84]\ttrain-logloss:0.259425\teval-logloss:0.263157\n",
      "[85]\ttrain-logloss:0.259228\teval-logloss:0.262987\n",
      "[86]\ttrain-logloss:0.259038\teval-logloss:0.262818\n",
      "[87]\ttrain-logloss:0.258914\teval-logloss:0.262725\n",
      "[88]\ttrain-logloss:0.258797\teval-logloss:0.262643\n",
      "[89]\ttrain-logloss:0.258709\teval-logloss:0.262569\n",
      "[90]\ttrain-logloss:0.258529\teval-logloss:0.262461\n",
      "[91]\ttrain-logloss:0.25841\teval-logloss:0.262381\n",
      "[92]\ttrain-logloss:0.258299\teval-logloss:0.262331\n",
      "[93]\ttrain-logloss:0.258074\teval-logloss:0.262135\n",
      "[94]\ttrain-logloss:0.257767\teval-logloss:0.261878\n",
      "[95]\ttrain-logloss:0.257672\teval-logloss:0.261806\n",
      "[96]\ttrain-logloss:0.257468\teval-logloss:0.26165\n",
      "[97]\ttrain-logloss:0.257343\teval-logloss:0.261544\n",
      "[98]\ttrain-logloss:0.257254\teval-logloss:0.261486\n",
      "[99]\ttrain-logloss:0.257042\teval-logloss:0.261316\n",
      "[100]\ttrain-logloss:0.256918\teval-logloss:0.261247\n",
      "[101]\ttrain-logloss:0.256815\teval-logloss:0.261164\n",
      "[102]\ttrain-logloss:0.25665\teval-logloss:0.261034\n",
      "[103]\ttrain-logloss:0.256571\teval-logloss:0.260986\n",
      "[104]\ttrain-logloss:0.256465\teval-logloss:0.260937\n",
      "[105]\ttrain-logloss:0.256294\teval-logloss:0.260822\n",
      "[106]\ttrain-logloss:0.256114\teval-logloss:0.260697\n",
      "[107]\ttrain-logloss:0.255969\teval-logloss:0.260574\n",
      "[108]\ttrain-logloss:0.25591\teval-logloss:0.260536\n",
      "[109]\ttrain-logloss:0.255678\teval-logloss:0.260323\n",
      "[110]\ttrain-logloss:0.255582\teval-logloss:0.26027\n",
      "[111]\ttrain-logloss:0.255511\teval-logloss:0.260237\n",
      "[112]\ttrain-logloss:0.255275\teval-logloss:0.260024\n",
      "[113]\ttrain-logloss:0.255124\teval-logloss:0.259881\n",
      "[114]\ttrain-logloss:0.255021\teval-logloss:0.259816\n",
      "[115]\ttrain-logloss:0.254953\teval-logloss:0.259773\n",
      "[116]\ttrain-logloss:0.25469\teval-logloss:0.259551\n",
      "[117]\ttrain-logloss:0.254587\teval-logloss:0.259464\n",
      "[118]\ttrain-logloss:0.25443\teval-logloss:0.259333\n",
      "[119]\ttrain-logloss:0.254323\teval-logloss:0.259246\n",
      "[120]\ttrain-logloss:0.254218\teval-logloss:0.259191\n",
      "[121]\ttrain-logloss:0.254113\teval-logloss:0.259123\n",
      "[122]\ttrain-logloss:0.253968\teval-logloss:0.259024\n",
      "[123]\ttrain-logloss:0.253847\teval-logloss:0.258956\n",
      "[124]\ttrain-logloss:0.253768\teval-logloss:0.258915\n",
      "[125]\ttrain-logloss:0.25372\teval-logloss:0.258887\n",
      "[126]\ttrain-logloss:0.253545\teval-logloss:0.258764\n",
      "[127]\ttrain-logloss:0.253343\teval-logloss:0.258596\n",
      "[128]\ttrain-logloss:0.253195\teval-logloss:0.258477\n",
      "[129]\ttrain-logloss:0.252991\teval-logloss:0.258299\n",
      "[130]\ttrain-logloss:0.252523\teval-logloss:0.257859\n",
      "[131]\ttrain-logloss:0.252435\teval-logloss:0.257783\n",
      "[132]\ttrain-logloss:0.252359\teval-logloss:0.257742\n",
      "[133]\ttrain-logloss:0.25227\teval-logloss:0.257697\n",
      "[134]\ttrain-logloss:0.252037\teval-logloss:0.257504\n",
      "[135]\ttrain-logloss:0.251982\teval-logloss:0.257472\n",
      "[136]\ttrain-logloss:0.251865\teval-logloss:0.25741\n",
      "[137]\ttrain-logloss:0.251818\teval-logloss:0.257381\n",
      "[138]\ttrain-logloss:0.251581\teval-logloss:0.257199\n",
      "[139]\ttrain-logloss:0.251327\teval-logloss:0.25698\n",
      "[140]\ttrain-logloss:0.25129\teval-logloss:0.256955\n",
      "[141]\ttrain-logloss:0.250955\teval-logloss:0.25665\n",
      "[142]\ttrain-logloss:0.250733\teval-logloss:0.25645\n",
      "[143]\ttrain-logloss:0.250509\teval-logloss:0.256259\n",
      "[144]\ttrain-logloss:0.250384\teval-logloss:0.256163\n",
      "[145]\ttrain-logloss:0.250138\teval-logloss:0.255964\n",
      "[146]\ttrain-logloss:0.249997\teval-logloss:0.255841\n",
      "[147]\ttrain-logloss:0.249911\teval-logloss:0.255789\n",
      "[148]\ttrain-logloss:0.24978\teval-logloss:0.255697\n",
      "[149]\ttrain-logloss:0.249613\teval-logloss:0.255581\n",
      "[150]\ttrain-logloss:0.249407\teval-logloss:0.25543\n",
      "[151]\ttrain-logloss:0.24927\teval-logloss:0.255351\n",
      "[152]\ttrain-logloss:0.249087\teval-logloss:0.25524\n",
      "[153]\ttrain-logloss:0.248996\teval-logloss:0.255171\n",
      "[154]\ttrain-logloss:0.248953\teval-logloss:0.255149\n",
      "[155]\ttrain-logloss:0.24887\teval-logloss:0.255119\n",
      "[156]\ttrain-logloss:0.248752\teval-logloss:0.255046\n",
      "[157]\ttrain-logloss:0.248552\teval-logloss:0.25489\n",
      "[158]\ttrain-logloss:0.248433\teval-logloss:0.254817\n",
      "[159]\ttrain-logloss:0.248419\teval-logloss:0.254813\n",
      "[160]\ttrain-logloss:0.248325\teval-logloss:0.254752\n",
      "[161]\ttrain-logloss:0.248289\teval-logloss:0.254734\n",
      "[162]\ttrain-logloss:0.248177\teval-logloss:0.254656\n",
      "[163]\ttrain-logloss:0.248093\teval-logloss:0.25461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\ttrain-logloss:0.247873\teval-logloss:0.254421\n",
      "[165]\ttrain-logloss:0.247826\teval-logloss:0.2544\n",
      "[166]\ttrain-logloss:0.247754\teval-logloss:0.254364\n",
      "[167]\ttrain-logloss:0.247712\teval-logloss:0.254337\n",
      "[168]\ttrain-logloss:0.247614\teval-logloss:0.254257\n",
      "[169]\ttrain-logloss:0.247598\teval-logloss:0.254252\n",
      "[170]\ttrain-logloss:0.247586\teval-logloss:0.254249\n",
      "[171]\ttrain-logloss:0.247504\teval-logloss:0.254193\n",
      "[172]\ttrain-logloss:0.247274\teval-logloss:0.254009\n",
      "[173]\ttrain-logloss:0.247228\teval-logloss:0.253986\n",
      "[174]\ttrain-logloss:0.247117\teval-logloss:0.253905\n",
      "[175]\ttrain-logloss:0.246981\teval-logloss:0.253823\n",
      "[176]\ttrain-logloss:0.246859\teval-logloss:0.253739\n",
      "[177]\ttrain-logloss:0.246688\teval-logloss:0.253634\n",
      "[178]\ttrain-logloss:0.246495\teval-logloss:0.253479\n",
      "[179]\ttrain-logloss:0.246421\teval-logloss:0.253428\n",
      "[180]\ttrain-logloss:0.246244\teval-logloss:0.253277\n",
      "[181]\ttrain-logloss:0.245979\teval-logloss:0.253044\n",
      "[182]\ttrain-logloss:0.245902\teval-logloss:0.252982\n",
      "[183]\ttrain-logloss:0.245888\teval-logloss:0.252972\n",
      "[184]\ttrain-logloss:0.245868\teval-logloss:0.252967\n",
      "[185]\ttrain-logloss:0.245843\teval-logloss:0.252952\n",
      "[186]\ttrain-logloss:0.245809\teval-logloss:0.252939\n",
      "[187]\ttrain-logloss:0.245721\teval-logloss:0.252896\n",
      "[188]\ttrain-logloss:0.245589\teval-logloss:0.252806\n",
      "[189]\ttrain-logloss:0.245449\teval-logloss:0.252726\n",
      "[190]\ttrain-logloss:0.245271\teval-logloss:0.252593\n",
      "[191]\ttrain-logloss:0.245259\teval-logloss:0.25259\n",
      "[192]\ttrain-logloss:0.245248\teval-logloss:0.252588\n",
      "[193]\ttrain-logloss:0.24523\teval-logloss:0.252582\n",
      "[194]\ttrain-logloss:0.245172\teval-logloss:0.252564\n",
      "[195]\ttrain-logloss:0.24512\teval-logloss:0.252526\n",
      "[196]\ttrain-logloss:0.245002\teval-logloss:0.252463\n",
      "[197]\ttrain-logloss:0.244971\teval-logloss:0.252447\n",
      "[198]\ttrain-logloss:0.244847\teval-logloss:0.252369\n",
      "[199]\ttrain-logloss:0.244824\teval-logloss:0.252362\n",
      "[200]\ttrain-logloss:0.244782\teval-logloss:0.252355\n",
      "[201]\ttrain-logloss:0.244772\teval-logloss:0.252348\n",
      "[202]\ttrain-logloss:0.244696\teval-logloss:0.25231\n",
      "[203]\ttrain-logloss:0.244656\teval-logloss:0.252288\n",
      "[204]\ttrain-logloss:0.244618\teval-logloss:0.252277\n",
      "[205]\ttrain-logloss:0.244466\teval-logloss:0.252175\n",
      "[206]\ttrain-logloss:0.244414\teval-logloss:0.252167\n",
      "[207]\ttrain-logloss:0.244395\teval-logloss:0.252158\n",
      "[208]\ttrain-logloss:0.244283\teval-logloss:0.252096\n",
      "[209]\ttrain-logloss:0.244156\teval-logloss:0.252017\n",
      "[210]\ttrain-logloss:0.244041\teval-logloss:0.25195\n",
      "[211]\ttrain-logloss:0.243937\teval-logloss:0.251881\n",
      "[212]\ttrain-logloss:0.243705\teval-logloss:0.251671\n",
      "[213]\ttrain-logloss:0.243696\teval-logloss:0.251672\n",
      "[214]\ttrain-logloss:0.243588\teval-logloss:0.251607\n",
      "[215]\ttrain-logloss:0.243453\teval-logloss:0.251524\n",
      "[216]\ttrain-logloss:0.24343\teval-logloss:0.251507\n",
      "[217]\ttrain-logloss:0.243403\teval-logloss:0.2515\n",
      "[218]\ttrain-logloss:0.243311\teval-logloss:0.251444\n",
      "[219]\ttrain-logloss:0.243252\teval-logloss:0.251422\n",
      "[220]\ttrain-logloss:0.243212\teval-logloss:0.251412\n",
      "[221]\ttrain-logloss:0.243191\teval-logloss:0.251395\n",
      "[222]\ttrain-logloss:0.243175\teval-logloss:0.251391\n",
      "[223]\ttrain-logloss:0.243131\teval-logloss:0.251372\n",
      "[224]\ttrain-logloss:0.243069\teval-logloss:0.251369\n",
      "[225]\ttrain-logloss:0.242946\teval-logloss:0.251303\n",
      "[226]\ttrain-logloss:0.242867\teval-logloss:0.251258\n",
      "[227]\ttrain-logloss:0.242828\teval-logloss:0.251239\n",
      "[228]\ttrain-logloss:0.242695\teval-logloss:0.251128\n",
      "[229]\ttrain-logloss:0.242581\teval-logloss:0.251031\n",
      "[230]\ttrain-logloss:0.24244\teval-logloss:0.250939\n",
      "[231]\ttrain-logloss:0.242419\teval-logloss:0.250934\n",
      "[232]\ttrain-logloss:0.242374\teval-logloss:0.250908\n",
      "[233]\ttrain-logloss:0.242301\teval-logloss:0.250848\n",
      "[234]\ttrain-logloss:0.242294\teval-logloss:0.250846\n",
      "[235]\ttrain-logloss:0.242274\teval-logloss:0.250842\n",
      "[236]\ttrain-logloss:0.242235\teval-logloss:0.250828\n",
      "[237]\ttrain-logloss:0.242093\teval-logloss:0.25072\n",
      "[238]\ttrain-logloss:0.242001\teval-logloss:0.250673\n",
      "[239]\ttrain-logloss:0.241994\teval-logloss:0.250674\n",
      "[240]\ttrain-logloss:0.241903\teval-logloss:0.250602\n",
      "[241]\ttrain-logloss:0.241801\teval-logloss:0.250554\n",
      "[242]\ttrain-logloss:0.241681\teval-logloss:0.250481\n",
      "[243]\ttrain-logloss:0.24155\teval-logloss:0.250389\n",
      "[244]\ttrain-logloss:0.241442\teval-logloss:0.25035\n",
      "[245]\ttrain-logloss:0.241383\teval-logloss:0.250333\n",
      "[246]\ttrain-logloss:0.24133\teval-logloss:0.250309\n",
      "[247]\ttrain-logloss:0.241295\teval-logloss:0.250298\n",
      "[248]\ttrain-logloss:0.241277\teval-logloss:0.250292\n",
      "[249]\ttrain-logloss:0.241251\teval-logloss:0.250284\n",
      "[250]\ttrain-logloss:0.241191\teval-logloss:0.250272\n",
      "[251]\ttrain-logloss:0.24107\teval-logloss:0.250206\n",
      "[252]\ttrain-logloss:0.240945\teval-logloss:0.250131\n",
      "[253]\ttrain-logloss:0.240863\teval-logloss:0.250077\n",
      "[254]\ttrain-logloss:0.240783\teval-logloss:0.250032\n",
      "[255]\ttrain-logloss:0.240775\teval-logloss:0.250029\n",
      "[256]\ttrain-logloss:0.240719\teval-logloss:0.250001\n",
      "[257]\ttrain-logloss:0.240669\teval-logloss:0.249976\n",
      "[258]\ttrain-logloss:0.240629\teval-logloss:0.249963\n",
      "[259]\ttrain-logloss:0.24057\teval-logloss:0.249922\n",
      "[260]\ttrain-logloss:0.240498\teval-logloss:0.249878\n",
      "[261]\ttrain-logloss:0.240382\teval-logloss:0.249812\n",
      "[262]\ttrain-logloss:0.240297\teval-logloss:0.249774\n",
      "[263]\ttrain-logloss:0.240251\teval-logloss:0.24976\n",
      "[264]\ttrain-logloss:0.24005\teval-logloss:0.249581\n",
      "[265]\ttrain-logloss:0.239967\teval-logloss:0.249519\n",
      "[266]\ttrain-logloss:0.239886\teval-logloss:0.24948\n",
      "[267]\ttrain-logloss:0.239874\teval-logloss:0.249476\n",
      "[268]\ttrain-logloss:0.239845\teval-logloss:0.249474\n",
      "[269]\ttrain-logloss:0.239771\teval-logloss:0.249421\n",
      "[270]\ttrain-logloss:0.239756\teval-logloss:0.249416\n",
      "[271]\ttrain-logloss:0.239738\teval-logloss:0.249416\n",
      "[272]\ttrain-logloss:0.239649\teval-logloss:0.249396\n",
      "[273]\ttrain-logloss:0.239592\teval-logloss:0.249369\n",
      "[274]\ttrain-logloss:0.239541\teval-logloss:0.249356\n",
      "[275]\ttrain-logloss:0.239384\teval-logloss:0.249242\n",
      "[276]\ttrain-logloss:0.23927\teval-logloss:0.249167\n",
      "[277]\ttrain-logloss:0.23919\teval-logloss:0.249126\n",
      "[278]\ttrain-logloss:0.239035\teval-logloss:0.248999\n",
      "[279]\ttrain-logloss:0.23897\teval-logloss:0.248957\n",
      "[280]\ttrain-logloss:0.238935\teval-logloss:0.248942\n",
      "[281]\ttrain-logloss:0.238903\teval-logloss:0.248933\n",
      "[282]\ttrain-logloss:0.238843\teval-logloss:0.248897\n",
      "[283]\ttrain-logloss:0.238808\teval-logloss:0.248893\n",
      "[284]\ttrain-logloss:0.238712\teval-logloss:0.248843\n",
      "[285]\ttrain-logloss:0.238636\teval-logloss:0.24882\n",
      "[286]\ttrain-logloss:0.238627\teval-logloss:0.248817\n",
      "[287]\ttrain-logloss:0.238597\teval-logloss:0.248807\n",
      "[288]\ttrain-logloss:0.238575\teval-logloss:0.248804\n",
      "[289]\ttrain-logloss:0.238549\teval-logloss:0.2488\n",
      "[290]\ttrain-logloss:0.238525\teval-logloss:0.248797\n",
      "[291]\ttrain-logloss:0.238445\teval-logloss:0.248759\n",
      "[292]\ttrain-logloss:0.238363\teval-logloss:0.24871\n",
      "[293]\ttrain-logloss:0.238328\teval-logloss:0.2487\n",
      "[294]\ttrain-logloss:0.238304\teval-logloss:0.248679\n",
      "[295]\ttrain-logloss:0.238201\teval-logloss:0.248628\n",
      "[296]\ttrain-logloss:0.238194\teval-logloss:0.248627\n",
      "[297]\ttrain-logloss:0.238167\teval-logloss:0.248624\n",
      "[298]\ttrain-logloss:0.238076\teval-logloss:0.248582\n",
      "[299]\ttrain-logloss:0.238002\teval-logloss:0.248534\n",
      "[300]\ttrain-logloss:0.237915\teval-logloss:0.24848\n",
      "[301]\ttrain-logloss:0.237852\teval-logloss:0.24847\n",
      "[302]\ttrain-logloss:0.237793\teval-logloss:0.248421\n",
      "[303]\ttrain-logloss:0.237735\teval-logloss:0.248383\n",
      "[304]\ttrain-logloss:0.237641\teval-logloss:0.248351\n",
      "[305]\ttrain-logloss:0.237534\teval-logloss:0.248302\n",
      "[306]\ttrain-logloss:0.237478\teval-logloss:0.248266\n",
      "[307]\ttrain-logloss:0.237465\teval-logloss:0.248257\n",
      "[308]\ttrain-logloss:0.237345\teval-logloss:0.248186\n",
      "[309]\ttrain-logloss:0.23726\teval-logloss:0.248145\n",
      "[310]\ttrain-logloss:0.23717\teval-logloss:0.248113\n",
      "[311]\ttrain-logloss:0.237125\teval-logloss:0.248094\n",
      "[312]\ttrain-logloss:0.237009\teval-logloss:0.248017\n",
      "[313]\ttrain-logloss:0.236964\teval-logloss:0.248005\n",
      "[314]\ttrain-logloss:0.236946\teval-logloss:0.248003\n",
      "[0]\ttrain-logloss:0.629497\teval-logloss:0.629398\n",
      "[1]\ttrain-logloss:0.578483\teval-logloss:0.578296\n",
      "[2]\ttrain-logloss:0.536716\teval-logloss:0.536473\n",
      "[3]\ttrain-logloss:0.501582\teval-logloss:0.501313\n",
      "[4]\ttrain-logloss:0.472288\teval-logloss:0.47201\n",
      "[5]\ttrain-logloss:0.447079\teval-logloss:0.446798\n",
      "[6]\ttrain-logloss:0.425789\teval-logloss:0.42553\n",
      "[7]\ttrain-logloss:0.407517\teval-logloss:0.407217\n",
      "[8]\ttrain-logloss:0.391918\teval-logloss:0.391611\n",
      "[9]\ttrain-logloss:0.377074\teval-logloss:0.376787\n",
      "[10]\ttrain-logloss:0.365575\teval-logloss:0.365308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\ttrain-logloss:0.354788\teval-logloss:0.354507\n",
      "[12]\ttrain-logloss:0.345914\teval-logloss:0.345645\n",
      "[13]\ttrain-logloss:0.337588\teval-logloss:0.337315\n",
      "[14]\ttrain-logloss:0.330385\teval-logloss:0.330119\n",
      "[15]\ttrain-logloss:0.323741\teval-logloss:0.323507\n",
      "[16]\ttrain-logloss:0.317577\teval-logloss:0.317408\n",
      "[17]\ttrain-logloss:0.312917\teval-logloss:0.312799\n",
      "[18]\ttrain-logloss:0.308143\teval-logloss:0.308036\n",
      "[19]\ttrain-logloss:0.303735\teval-logloss:0.303669\n",
      "[20]\ttrain-logloss:0.300246\teval-logloss:0.300218\n",
      "[21]\ttrain-logloss:0.297374\teval-logloss:0.297417\n",
      "[22]\ttrain-logloss:0.294652\teval-logloss:0.294734\n",
      "[23]\ttrain-logloss:0.292042\teval-logloss:0.292147\n",
      "[24]\ttrain-logloss:0.290075\teval-logloss:0.290213\n",
      "[25]\ttrain-logloss:0.287769\teval-logloss:0.28793\n",
      "[26]\ttrain-logloss:0.28613\teval-logloss:0.286354\n",
      "[27]\ttrain-logloss:0.284569\teval-logloss:0.284878\n",
      "[28]\ttrain-logloss:0.28304\teval-logloss:0.283375\n",
      "[29]\ttrain-logloss:0.281719\teval-logloss:0.282129\n",
      "[30]\ttrain-logloss:0.280512\teval-logloss:0.280972\n",
      "[31]\ttrain-logloss:0.279542\teval-logloss:0.280032\n",
      "[32]\ttrain-logloss:0.278704\teval-logloss:0.279261\n",
      "[33]\ttrain-logloss:0.277657\teval-logloss:0.278252\n",
      "[34]\ttrain-logloss:0.276808\teval-logloss:0.277438\n",
      "[35]\ttrain-logloss:0.275936\teval-logloss:0.276614\n",
      "[36]\ttrain-logloss:0.275135\teval-logloss:0.275853\n",
      "[37]\ttrain-logloss:0.274571\teval-logloss:0.275329\n",
      "[38]\ttrain-logloss:0.273862\teval-logloss:0.274652\n",
      "[39]\ttrain-logloss:0.273346\teval-logloss:0.274181\n",
      "[40]\ttrain-logloss:0.272786\teval-logloss:0.273654\n",
      "[41]\ttrain-logloss:0.272351\teval-logloss:0.273248\n",
      "[42]\ttrain-logloss:0.271886\teval-logloss:0.272819\n",
      "[43]\ttrain-logloss:0.271472\teval-logloss:0.272427\n",
      "[44]\ttrain-logloss:0.270884\teval-logloss:0.271868\n",
      "[45]\ttrain-logloss:0.270387\teval-logloss:0.271409\n",
      "[46]\ttrain-logloss:0.270026\teval-logloss:0.271103\n",
      "[47]\ttrain-logloss:0.269565\teval-logloss:0.270673\n",
      "[48]\ttrain-logloss:0.269027\teval-logloss:0.270232\n",
      "[49]\ttrain-logloss:0.268771\teval-logloss:0.270008\n",
      "[50]\ttrain-logloss:0.268431\teval-logloss:0.269702\n",
      "[51]\ttrain-logloss:0.268207\teval-logloss:0.269499\n",
      "[52]\ttrain-logloss:0.267897\teval-logloss:0.269239\n",
      "[53]\ttrain-logloss:0.26757\teval-logloss:0.268978\n",
      "[54]\ttrain-logloss:0.267348\teval-logloss:0.268773\n",
      "[55]\ttrain-logloss:0.267197\teval-logloss:0.268648\n",
      "[56]\ttrain-logloss:0.266912\teval-logloss:0.268416\n",
      "[57]\ttrain-logloss:0.266696\teval-logloss:0.268243\n",
      "[58]\ttrain-logloss:0.266367\teval-logloss:0.267949\n",
      "[59]\ttrain-logloss:0.266037\teval-logloss:0.267681\n",
      "[60]\ttrain-logloss:0.265591\teval-logloss:0.267268\n",
      "[61]\ttrain-logloss:0.265441\teval-logloss:0.26715\n",
      "[62]\ttrain-logloss:0.265195\teval-logloss:0.266939\n",
      "[63]\ttrain-logloss:0.264902\teval-logloss:0.266716\n",
      "[64]\ttrain-logloss:0.264753\teval-logloss:0.266584\n",
      "[65]\ttrain-logloss:0.264482\teval-logloss:0.26637\n",
      "[66]\ttrain-logloss:0.264294\teval-logloss:0.266237\n",
      "[67]\ttrain-logloss:0.264081\teval-logloss:0.266089\n",
      "[68]\ttrain-logloss:0.263603\teval-logloss:0.265654\n",
      "[69]\ttrain-logloss:0.263376\teval-logloss:0.265464\n",
      "[70]\ttrain-logloss:0.263113\teval-logloss:0.265253\n",
      "[71]\ttrain-logloss:0.262793\teval-logloss:0.264964\n",
      "[72]\ttrain-logloss:0.262627\teval-logloss:0.264831\n",
      "[73]\ttrain-logloss:0.262399\teval-logloss:0.26465\n",
      "[74]\ttrain-logloss:0.262203\teval-logloss:0.264512\n",
      "[75]\ttrain-logloss:0.262073\teval-logloss:0.264412\n",
      "[76]\ttrain-logloss:0.261828\teval-logloss:0.264249\n",
      "[77]\ttrain-logloss:0.261703\teval-logloss:0.264162\n",
      "[78]\ttrain-logloss:0.261449\teval-logloss:0.263938\n",
      "[79]\ttrain-logloss:0.261238\teval-logloss:0.26379\n",
      "[80]\ttrain-logloss:0.261112\teval-logloss:0.263678\n",
      "[81]\ttrain-logloss:0.260755\teval-logloss:0.26336\n",
      "[82]\ttrain-logloss:0.260663\teval-logloss:0.263284\n",
      "[83]\ttrain-logloss:0.260406\teval-logloss:0.263101\n",
      "[84]\ttrain-logloss:0.260244\teval-logloss:0.262976\n",
      "[85]\ttrain-logloss:0.259947\teval-logloss:0.26274\n",
      "[86]\ttrain-logloss:0.259793\teval-logloss:0.262607\n",
      "[87]\ttrain-logloss:0.259622\teval-logloss:0.26249\n",
      "[88]\ttrain-logloss:0.259484\teval-logloss:0.262379\n",
      "[89]\ttrain-logloss:0.259419\teval-logloss:0.262333\n",
      "[90]\ttrain-logloss:0.259242\teval-logloss:0.262192\n",
      "[91]\ttrain-logloss:0.258928\teval-logloss:0.261909\n",
      "[92]\ttrain-logloss:0.258828\teval-logloss:0.261843\n",
      "[93]\ttrain-logloss:0.258683\teval-logloss:0.261741\n",
      "[94]\ttrain-logloss:0.25846\teval-logloss:0.261521\n",
      "[95]\ttrain-logloss:0.258366\teval-logloss:0.261457\n",
      "[96]\ttrain-logloss:0.258146\teval-logloss:0.261238\n",
      "[97]\ttrain-logloss:0.258014\teval-logloss:0.261149\n",
      "[98]\ttrain-logloss:0.25765\teval-logloss:0.260829\n",
      "[99]\ttrain-logloss:0.257472\teval-logloss:0.260721\n",
      "[100]\ttrain-logloss:0.257281\teval-logloss:0.260567\n",
      "[101]\ttrain-logloss:0.257125\teval-logloss:0.260468\n",
      "[102]\ttrain-logloss:0.257014\teval-logloss:0.260396\n",
      "[103]\ttrain-logloss:0.25694\teval-logloss:0.26035\n",
      "[104]\ttrain-logloss:0.256842\teval-logloss:0.260291\n",
      "[105]\ttrain-logloss:0.256495\teval-logloss:0.259998\n",
      "[106]\ttrain-logloss:0.25631\teval-logloss:0.259817\n",
      "[107]\ttrain-logloss:0.256151\teval-logloss:0.259708\n",
      "[108]\ttrain-logloss:0.255917\teval-logloss:0.259517\n",
      "[109]\ttrain-logloss:0.25584\teval-logloss:0.259465\n",
      "[110]\ttrain-logloss:0.255771\teval-logloss:0.259428\n",
      "[111]\ttrain-logloss:0.25567\teval-logloss:0.259346\n",
      "[112]\ttrain-logloss:0.255551\teval-logloss:0.25927\n",
      "[113]\ttrain-logloss:0.255399\teval-logloss:0.259126\n",
      "[114]\ttrain-logloss:0.255313\teval-logloss:0.259073\n",
      "[115]\ttrain-logloss:0.255244\teval-logloss:0.259034\n",
      "[116]\ttrain-logloss:0.25516\teval-logloss:0.258994\n",
      "[117]\ttrain-logloss:0.254978\teval-logloss:0.258855\n",
      "[118]\ttrain-logloss:0.254892\teval-logloss:0.258805\n",
      "[119]\ttrain-logloss:0.254639\teval-logloss:0.258595\n",
      "[120]\ttrain-logloss:0.254504\teval-logloss:0.258496\n",
      "[121]\ttrain-logloss:0.254303\teval-logloss:0.258354\n",
      "[122]\ttrain-logloss:0.254211\teval-logloss:0.258289\n",
      "[123]\ttrain-logloss:0.254126\teval-logloss:0.25826\n",
      "[124]\ttrain-logloss:0.254088\teval-logloss:0.25824\n",
      "[125]\ttrain-logloss:0.253931\teval-logloss:0.258146\n",
      "[126]\ttrain-logloss:0.253812\teval-logloss:0.258044\n",
      "[127]\ttrain-logloss:0.253725\teval-logloss:0.257983\n",
      "[128]\ttrain-logloss:0.253641\teval-logloss:0.257931\n",
      "[129]\ttrain-logloss:0.253372\teval-logloss:0.257714\n",
      "[130]\ttrain-logloss:0.253226\teval-logloss:0.25758\n",
      "[131]\ttrain-logloss:0.253114\teval-logloss:0.257511\n",
      "[132]\ttrain-logloss:0.253059\teval-logloss:0.257479\n",
      "[133]\ttrain-logloss:0.252911\teval-logloss:0.257362\n",
      "[134]\ttrain-logloss:0.252584\teval-logloss:0.257061\n",
      "[135]\ttrain-logloss:0.252459\teval-logloss:0.256948\n",
      "[136]\ttrain-logloss:0.252438\teval-logloss:0.256935\n",
      "[137]\ttrain-logloss:0.252304\teval-logloss:0.256869\n",
      "[138]\ttrain-logloss:0.252195\teval-logloss:0.256814\n",
      "[139]\ttrain-logloss:0.252168\teval-logloss:0.256798\n",
      "[140]\ttrain-logloss:0.252068\teval-logloss:0.256722\n",
      "[141]\ttrain-logloss:0.251845\teval-logloss:0.256536\n",
      "[142]\ttrain-logloss:0.251666\teval-logloss:0.256404\n",
      "[143]\ttrain-logloss:0.25149\teval-logloss:0.256279\n",
      "[144]\ttrain-logloss:0.251317\teval-logloss:0.256151\n",
      "[145]\ttrain-logloss:0.251248\teval-logloss:0.256097\n",
      "[146]\ttrain-logloss:0.251192\teval-logloss:0.256062\n",
      "[147]\ttrain-logloss:0.250967\teval-logloss:0.255878\n",
      "[148]\ttrain-logloss:0.250796\teval-logloss:0.255755\n",
      "[149]\ttrain-logloss:0.250692\teval-logloss:0.255667\n",
      "[150]\ttrain-logloss:0.250649\teval-logloss:0.255653\n",
      "[151]\ttrain-logloss:0.250601\teval-logloss:0.255624\n",
      "[152]\ttrain-logloss:0.250519\teval-logloss:0.255572\n",
      "[153]\ttrain-logloss:0.250475\teval-logloss:0.255535\n",
      "[154]\ttrain-logloss:0.250262\teval-logloss:0.255338\n",
      "[155]\ttrain-logloss:0.250045\teval-logloss:0.255172\n",
      "[156]\ttrain-logloss:0.24999\teval-logloss:0.255147\n",
      "[157]\ttrain-logloss:0.249933\teval-logloss:0.255105\n",
      "[158]\ttrain-logloss:0.249843\teval-logloss:0.255058\n",
      "[159]\ttrain-logloss:0.249731\teval-logloss:0.254977\n",
      "[160]\ttrain-logloss:0.249691\teval-logloss:0.254961\n",
      "[161]\ttrain-logloss:0.249401\teval-logloss:0.254706\n",
      "[162]\ttrain-logloss:0.249231\teval-logloss:0.254579\n",
      "[163]\ttrain-logloss:0.249169\teval-logloss:0.254557\n",
      "[164]\ttrain-logloss:0.249113\teval-logloss:0.254529\n",
      "[165]\ttrain-logloss:0.249084\teval-logloss:0.254515\n",
      "[166]\ttrain-logloss:0.249042\teval-logloss:0.254494\n",
      "[167]\ttrain-logloss:0.248998\teval-logloss:0.254475\n",
      "[168]\ttrain-logloss:0.248945\teval-logloss:0.254453\n",
      "[169]\ttrain-logloss:0.248807\teval-logloss:0.254357\n",
      "[170]\ttrain-logloss:0.248768\teval-logloss:0.25432\n",
      "[171]\ttrain-logloss:0.24868\teval-logloss:0.254279\n",
      "[172]\ttrain-logloss:0.248467\teval-logloss:0.254105\n",
      "[173]\ttrain-logloss:0.248448\teval-logloss:0.254093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174]\ttrain-logloss:0.248158\teval-logloss:0.253825\n",
      "[175]\ttrain-logloss:0.248035\teval-logloss:0.253743\n",
      "[176]\ttrain-logloss:0.247919\teval-logloss:0.253656\n",
      "[177]\ttrain-logloss:0.247836\teval-logloss:0.253593\n",
      "[178]\ttrain-logloss:0.247681\teval-logloss:0.25349\n",
      "[179]\ttrain-logloss:0.247554\teval-logloss:0.253409\n",
      "[180]\ttrain-logloss:0.247466\teval-logloss:0.253348\n",
      "[181]\ttrain-logloss:0.247438\teval-logloss:0.253336\n",
      "[182]\ttrain-logloss:0.247394\teval-logloss:0.253324\n",
      "[183]\ttrain-logloss:0.247308\teval-logloss:0.253277\n",
      "[184]\ttrain-logloss:0.247201\teval-logloss:0.253213\n",
      "[185]\ttrain-logloss:0.247001\teval-logloss:0.253046\n",
      "[186]\ttrain-logloss:0.246858\teval-logloss:0.252939\n",
      "[187]\ttrain-logloss:0.246746\teval-logloss:0.252877\n",
      "[188]\ttrain-logloss:0.24672\teval-logloss:0.252867\n",
      "[189]\ttrain-logloss:0.246691\teval-logloss:0.252848\n",
      "[190]\ttrain-logloss:0.246637\teval-logloss:0.252815\n",
      "[191]\ttrain-logloss:0.246625\teval-logloss:0.252813\n",
      "[192]\ttrain-logloss:0.246581\teval-logloss:0.252792\n",
      "[193]\ttrain-logloss:0.246481\teval-logloss:0.252738\n",
      "[194]\ttrain-logloss:0.246447\teval-logloss:0.252733\n",
      "[195]\ttrain-logloss:0.246411\teval-logloss:0.252711\n",
      "[196]\ttrain-logloss:0.246402\teval-logloss:0.252708\n",
      "[197]\ttrain-logloss:0.246345\teval-logloss:0.252677\n",
      "[198]\ttrain-logloss:0.246322\teval-logloss:0.252669\n",
      "[199]\ttrain-logloss:0.246266\teval-logloss:0.25266\n",
      "[200]\ttrain-logloss:0.246108\teval-logloss:0.252544\n",
      "[201]\ttrain-logloss:0.246063\teval-logloss:0.252527\n",
      "[202]\ttrain-logloss:0.245889\teval-logloss:0.252392\n",
      "[203]\ttrain-logloss:0.245826\teval-logloss:0.252366\n",
      "[204]\ttrain-logloss:0.245802\teval-logloss:0.252352\n",
      "[205]\ttrain-logloss:0.245714\teval-logloss:0.25231\n",
      "[206]\ttrain-logloss:0.245566\teval-logloss:0.252209\n",
      "[207]\ttrain-logloss:0.245526\teval-logloss:0.252193\n",
      "[208]\ttrain-logloss:0.245456\teval-logloss:0.252145\n",
      "[209]\ttrain-logloss:0.245351\teval-logloss:0.252087\n",
      "[210]\ttrain-logloss:0.245295\teval-logloss:0.252072\n",
      "[211]\ttrain-logloss:0.245244\teval-logloss:0.252053\n",
      "[212]\ttrain-logloss:0.245228\teval-logloss:0.252051\n",
      "[213]\ttrain-logloss:0.245209\teval-logloss:0.252048\n",
      "[214]\ttrain-logloss:0.245088\teval-logloss:0.25197\n",
      "[215]\ttrain-logloss:0.245041\teval-logloss:0.251931\n",
      "[216]\ttrain-logloss:0.244972\teval-logloss:0.251885\n",
      "[217]\ttrain-logloss:0.244948\teval-logloss:0.251869\n",
      "[218]\ttrain-logloss:0.24493\teval-logloss:0.251865\n",
      "[219]\ttrain-logloss:0.244818\teval-logloss:0.251797\n",
      "[220]\ttrain-logloss:0.244784\teval-logloss:0.25178\n",
      "[221]\ttrain-logloss:0.244771\teval-logloss:0.251776\n",
      "[222]\ttrain-logloss:0.244701\teval-logloss:0.25174\n",
      "[223]\ttrain-logloss:0.244545\teval-logloss:0.251635\n",
      "[224]\ttrain-logloss:0.2445\teval-logloss:0.251625\n",
      "[225]\ttrain-logloss:0.24445\teval-logloss:0.251613\n",
      "[226]\ttrain-logloss:0.244429\teval-logloss:0.251604\n",
      "[227]\ttrain-logloss:0.24432\teval-logloss:0.251541\n",
      "[228]\ttrain-logloss:0.244179\teval-logloss:0.251458\n",
      "[229]\ttrain-logloss:0.244088\teval-logloss:0.251426\n",
      "[230]\ttrain-logloss:0.244044\teval-logloss:0.251417\n",
      "[231]\ttrain-logloss:0.243898\teval-logloss:0.251327\n",
      "[232]\ttrain-logloss:0.243643\teval-logloss:0.251125\n",
      "[233]\ttrain-logloss:0.243506\teval-logloss:0.251016\n",
      "[234]\ttrain-logloss:0.243294\teval-logloss:0.25085\n",
      "[235]\ttrain-logloss:0.243203\teval-logloss:0.250786\n",
      "[236]\ttrain-logloss:0.24314\teval-logloss:0.250758\n",
      "[237]\ttrain-logloss:0.243081\teval-logloss:0.250716\n",
      "[238]\ttrain-logloss:0.243062\teval-logloss:0.250714\n",
      "[239]\ttrain-logloss:0.24295\teval-logloss:0.250639\n",
      "[240]\ttrain-logloss:0.242841\teval-logloss:0.250599\n",
      "[241]\ttrain-logloss:0.242733\teval-logloss:0.250543\n",
      "[242]\ttrain-logloss:0.242636\teval-logloss:0.25049\n",
      "[243]\ttrain-logloss:0.242573\teval-logloss:0.250453\n",
      "[244]\ttrain-logloss:0.242518\teval-logloss:0.250436\n",
      "[245]\ttrain-logloss:0.242507\teval-logloss:0.250432\n",
      "[246]\ttrain-logloss:0.242452\teval-logloss:0.250413\n",
      "[247]\ttrain-logloss:0.242422\teval-logloss:0.250406\n",
      "[248]\ttrain-logloss:0.242404\teval-logloss:0.250393\n",
      "[249]\ttrain-logloss:0.242252\teval-logloss:0.250264\n",
      "[250]\ttrain-logloss:0.242181\teval-logloss:0.25024\n",
      "[251]\ttrain-logloss:0.242152\teval-logloss:0.250232\n",
      "[252]\ttrain-logloss:0.242107\teval-logloss:0.250215\n",
      "[253]\ttrain-logloss:0.241995\teval-logloss:0.250146\n",
      "[254]\ttrain-logloss:0.241982\teval-logloss:0.250138\n",
      "[255]\ttrain-logloss:0.241866\teval-logloss:0.250074\n",
      "[256]\ttrain-logloss:0.241793\teval-logloss:0.250016\n",
      "[257]\ttrain-logloss:0.241775\teval-logloss:0.250008\n",
      "[258]\ttrain-logloss:0.241763\teval-logloss:0.250003\n",
      "[259]\ttrain-logloss:0.241678\teval-logloss:0.249977\n",
      "[260]\ttrain-logloss:0.241666\teval-logloss:0.249973\n",
      "[261]\ttrain-logloss:0.241632\teval-logloss:0.249961\n",
      "[262]\ttrain-logloss:0.241542\teval-logloss:0.249889\n",
      "[263]\ttrain-logloss:0.241486\teval-logloss:0.249851\n",
      "[264]\ttrain-logloss:0.241355\teval-logloss:0.249756\n",
      "[265]\ttrain-logloss:0.241269\teval-logloss:0.249706\n",
      "[266]\ttrain-logloss:0.241208\teval-logloss:0.249687\n",
      "[267]\ttrain-logloss:0.241125\teval-logloss:0.24965\n",
      "[268]\ttrain-logloss:0.241027\teval-logloss:0.24961\n",
      "[269]\ttrain-logloss:0.240939\teval-logloss:0.249568\n",
      "[270]\ttrain-logloss:0.240857\teval-logloss:0.249526\n",
      "[271]\ttrain-logloss:0.240773\teval-logloss:0.249486\n",
      "[272]\ttrain-logloss:0.240756\teval-logloss:0.249484\n",
      "[273]\ttrain-logloss:0.24063\teval-logloss:0.249397\n",
      "[274]\ttrain-logloss:0.240554\teval-logloss:0.249365\n",
      "[275]\ttrain-logloss:0.240512\teval-logloss:0.249356\n",
      "[276]\ttrain-logloss:0.240481\teval-logloss:0.249337\n",
      "[277]\ttrain-logloss:0.240456\teval-logloss:0.249323\n",
      "[278]\ttrain-logloss:0.240442\teval-logloss:0.249324\n",
      "[279]\ttrain-logloss:0.240366\teval-logloss:0.249283\n",
      "[280]\ttrain-logloss:0.240318\teval-logloss:0.249266\n",
      "[281]\ttrain-logloss:0.240221\teval-logloss:0.249222\n",
      "[282]\ttrain-logloss:0.240213\teval-logloss:0.249216\n",
      "[283]\ttrain-logloss:0.240182\teval-logloss:0.24921\n",
      "[284]\ttrain-logloss:0.240155\teval-logloss:0.249205\n",
      "[285]\ttrain-logloss:0.240046\teval-logloss:0.249134\n",
      "[286]\ttrain-logloss:0.240029\teval-logloss:0.249127\n",
      "[287]\ttrain-logloss:0.239938\teval-logloss:0.249077\n",
      "[288]\ttrain-logloss:0.239863\teval-logloss:0.249046\n",
      "[289]\ttrain-logloss:0.239707\teval-logloss:0.248927\n",
      "[290]\ttrain-logloss:0.239656\teval-logloss:0.248902\n",
      "[291]\ttrain-logloss:0.239644\teval-logloss:0.248901\n",
      "[292]\ttrain-logloss:0.239535\teval-logloss:0.248835\n",
      "[293]\ttrain-logloss:0.239435\teval-logloss:0.248791\n",
      "[294]\ttrain-logloss:0.239342\teval-logloss:0.248742\n",
      "[295]\ttrain-logloss:0.239266\teval-logloss:0.248714\n",
      "[296]\ttrain-logloss:0.239174\teval-logloss:0.248648\n",
      "[297]\ttrain-logloss:0.239129\teval-logloss:0.248639\n",
      "[298]\ttrain-logloss:0.239095\teval-logloss:0.248626\n",
      "[299]\ttrain-logloss:0.239079\teval-logloss:0.248626\n",
      "[300]\ttrain-logloss:0.238993\teval-logloss:0.248582\n",
      "[301]\ttrain-logloss:0.238896\teval-logloss:0.248528\n",
      "[302]\ttrain-logloss:0.238801\teval-logloss:0.248465\n",
      "[303]\ttrain-logloss:0.238706\teval-logloss:0.248395\n",
      "[304]\ttrain-logloss:0.238622\teval-logloss:0.248343\n",
      "[305]\ttrain-logloss:0.238551\teval-logloss:0.248305\n",
      "[306]\ttrain-logloss:0.238468\teval-logloss:0.248265\n",
      "[307]\ttrain-logloss:0.238368\teval-logloss:0.248235\n",
      "[308]\ttrain-logloss:0.238224\teval-logloss:0.248132\n",
      "[309]\ttrain-logloss:0.238162\teval-logloss:0.248107\n",
      "[310]\ttrain-logloss:0.23806\teval-logloss:0.248035\n",
      "[311]\ttrain-logloss:0.237954\teval-logloss:0.24795\n",
      "[312]\ttrain-logloss:0.237918\teval-logloss:0.247935\n",
      "[313]\ttrain-logloss:0.237846\teval-logloss:0.247902\n",
      "[314]\ttrain-logloss:0.237767\teval-logloss:0.247851\n",
      "CPU times: user 9min 51s, sys: 5.51 s, total: 9min 56s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "### XGBoost ###\n",
    "###############\n",
    "\n",
    "%%time\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.11\n",
    "params['max_depth'] = 5\n",
    "params['silent'] = 1\n",
    "params['seed'] = RS\n",
    "\n",
    "layer_1_train, layer_1_val = get_2fold_stack(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### Logistic Regression ###\n",
    "###########################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# train_fold1_scaled = scaler.fit_transform(train_fold1)\n",
    "# train_fold2_scaled = scaler.fit_transform(train_fold2)\n",
    "# validation_fold_scaled = scaler.fit_transform(validation_fold)\n",
    "\n",
    "def get_2fold_stack_logit(model):\n",
    "    model.fit(train_fold1_scaled, y_train_fold1)\n",
    "    new_fold2 = model.predict_proba(train_fold2_scaled)\n",
    "    v1 = model.predict_proba(validation_fold_scaled)  ### There is model\n",
    "\n",
    "    model.fit(train_fold2_scaled, y_train_fold2)\n",
    "    new_fold1 = model.predict_proba(train_fold1_scaled)\n",
    "    v2 = model.predict_proba(validation_fold_scaled) ### The model here is different\n",
    "    \n",
    "    return np.concatenate([new_fold1, new_fold2], axis=0), (v1+v2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear]CPU times: user 7.39 s, sys: 187 ms, total: 7.57 s\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logit = LogisticRegression(verbose=10)\n",
    "logit_layer_1_train, logit_layer_1_val = get_2fold_stack_logit(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is: 0.439\n"
     ]
    }
   ],
   "source": [
    "print('The log loss is: %.3f' % log_loss(y_validation_fold, logit_layer_1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### Factorized Machine ###\n",
    "##########################\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csc_matrix\n",
    "from fastFM import sgd, mcmc\n",
    "from fastFM.datasets import make_user_item_regression\n",
    "\n",
    "\n",
    "# train_fold1 = x_train.loc[fold1_index.values.flatten(), :]\n",
    "# train_fold2 = x_train.loc[fold2_index.values.flatten(), :]\n",
    "# validation_fold = x_train.loc[validation_index.values.flatten(), :]\n",
    "\n",
    "# y_train_fold1 = y_train.loc[fold1_index.values.flatten(), :]\n",
    "# y_train_fold2 = y_train.loc[fold2_index.values.flatten(), :]\n",
    "# y_validation_fold = y_train.loc[validation_index.values.flatten(), :]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_fold1_scaled = scaler.fit_transform(train_fold1)\n",
    "train_fold2_scaled = scaler.fit_transform(train_fold2)\n",
    "validation_fold_scaled = scaler.fit_transform(validation_fold)\n",
    "\n",
    "n_iter = 1000\n",
    "step_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = sgd.FMClassification(n_iter=1000, init_stdev=0.1, l2_reg_w=0.8,\n",
    "                          l2_reg_V=0.8, rank=2, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fm_mcmc = mcmc.FMClassification(n_iter=1000, init_stdev=0.1, rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    210408\n",
      "1.0     49755\n",
      "Name: 0, dtype: int64\n",
      "-1.0    210408\n",
      " 1.0     49755\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y_labels = np.ones_like(y_train_fold1['0'])\n",
    "y_labels[y_train_fold1['0'] < np.mean(y_train_fold1['0'])] = -1\n",
    "\n",
    "print(y_train_fold1['0'].value_counts())\n",
    "print(pd.Series(data=y_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "new_fold2 = fm_mcmc.fit_predict_proba(csc_matrix(train_fold1_scaled), y_labels, csc_matrix(train_fold2_scaled))\n",
    "v1 = fm_mcmc.fit_predict_proba(csc_matrix(train_fold1_scaled), y_labels, csc_matrix(validation_fold_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logloss_train = []\n",
    "logloss_val = []\n",
    "for i in range(1, n_iter):\n",
    "    fm.fit(csc_matrix(train_fold1_scaled), y_labels)\n",
    "    new_fold1 = fm.predict_proba(csc_matrix(train_fold1_scaled))\n",
    "    v1 = fm.predict_proba(csc_matrix(validation_fold_scaled))\n",
    "    \n",
    "    logloss_train.append(log_loss(y_train_fold1, new_fold1))\n",
    "    logloss_val.append(log_loss(y_validation_fold, v1))\n",
    "\n",
    "# fm.fit(csc_matrix(train_fold1_scaled), y_labels)\n",
    "# new_fold2 = fm.predict_proba(csc_matrix(train_fold2_scaled))\n",
    "# v1 = fm.predict_proba(csc_matrix(validation_fold_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 4))\n",
    "\n",
    "x = np.arange(1, n_iter) * step_size\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    axes[0].plot(x, logloss_train, label='logloss-train', color='r', ls=\"--\")\n",
    "    axes[0].plot(x, logloss_val, label='logloss-test', color='r')\n",
    "axes[0].set_ylabel('logloss', color='r')\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.34532516261\n",
      "3.34121657739\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_train_fold2, new_fold2))\n",
    "print(log_loss(y_validation_fold, v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### Factorized Machine ###\n",
    "##########################\n",
    "\n",
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt,pow\n",
    "import itertools\n",
    "import math\n",
    "from random import random,shuffle,uniform,seed\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(path, no_norm=False, task='c'):\n",
    "    ''' Unfold compressed data into matrix.'''\n",
    "    data = open(path,'r')\n",
    "    for row in data:\n",
    "        row = row.strip().split(\" \")\n",
    "        y = float(row[0])\n",
    "        row = row[1:]\n",
    "        x = []\n",
    "        for feature in row:\n",
    "            feature = feature.split(\":\")\n",
    "            idx = int(feature[0])\n",
    "            value = float(feature[1])\n",
    "            x.append([idx,value])\n",
    "\n",
    "        if not no_norm:\n",
    "            r = 0.0\n",
    "            for i in range(len(x)):\n",
    "                r+=x[i][1]*x[i][1]\n",
    "            for i in range(len(x)):\n",
    "                x[i][1] /=r\n",
    "        # if task=='c':\n",
    "        #     if y ==0.0:\n",
    "        #         y = -1.0\n",
    "\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def dot(u,v):\n",
    "    u_v = 0.\n",
    "    len_u = len(u)\n",
    "    for idx in range(len_u):\n",
    "        uu = u[idx]\n",
    "        vv = v[idx]\n",
    "        u_v += uu*vv\n",
    "    return u_v\n",
    "\n",
    "def mse_loss_function(y,p):\n",
    "    return (y - p)**2\n",
    "\n",
    "def mae_loss_function(y,p):\n",
    "    y = exp(y)\n",
    "    p = exp(p)\n",
    "    return abs(y - p)\n",
    "\n",
    "def log_loss_function(y,p):\n",
    "    return -(y*log(p)+(1-y)*log(1-p))\n",
    "\n",
    "def exponential_loss_function(y,p):\n",
    "    return log(1+exp(-y*p))\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1/(1+exp(-inX))\n",
    "\n",
    "def bounded_sigmoid(inX):\n",
    "    return 1. / (1. + exp(-max(min(inX, 35.), -35.)))\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, lr=0.001, momentum=0.9, nesterov=True, adam=False, l2=0.0, \\\n",
    "                 l2_fm=0.0, l2_bias=0.0, ini_stdev= 0.01, dropout=0.5, task='c', \\\n",
    "                 n_components=4, nb_epoch=5, interaction=False, no_norm=False):\n",
    "\n",
    "        self.W = []\n",
    "        self.V = []        \n",
    "        self.bias = uniform(-ini_stdev, ini_stdev)\n",
    "        self.n_components=n_components\n",
    "        self.lr = lr # learning rate\n",
    "        self.l2 = l2\n",
    "        self.l2_fm = l2_fm\n",
    "        self.l2_bias = l2_bias\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.adam = adam\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.ini_stdev = ini_stdev\n",
    "        self.task = task\n",
    "        self.interaction = interaction\n",
    "        self.dropout = dropout\n",
    "        self.no_norm = no_norm\n",
    "        if self.task!='c':\n",
    "            # self.loss_function = mse_loss_function\n",
    "            self.loss_function = mae_loss_function\n",
    "        else:\n",
    "            # self.loss_function = exponential_loss_function\n",
    "            self.loss_function = log_loss_function\n",
    "\n",
    "    def preload(self, train, test):\n",
    "        train = data_generator(train, self.no_norm, self.task)\n",
    "        dim = 0\n",
    "        count = 0\n",
    "        for x, y in train:\n",
    "            for i in x:\n",
    "                idx, value = i\n",
    "                if idx > dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Training samples:',count)\n",
    "        test = data_generator(test,self.no_norm,self.task)\n",
    "        count=0\n",
    "        for x,y in test:\n",
    "            for i in x:\n",
    "                idx,value = i\n",
    "                if idx >dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Testing samples:', count)\n",
    "        \n",
    "        dim = dim + 1\n",
    "        print(\"Number of features:\", dim)\n",
    "        \n",
    "        self.W = [uniform(-self.ini_stdev, self.ini_stdev) for _ in range(dim)]\n",
    "        self.Velocity_W = [0.0 for _ in range(dim)]\n",
    "        \n",
    "        \n",
    "        self.V = [[uniform(-self.ini_stdev, self.ini_stdev) for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.Velocity_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        \n",
    "        self.Velocity_bias = 0.0\n",
    "        \n",
    "        self.dim = dim\n",
    "        \n",
    "        \n",
    "    def adam_init(self):\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.decay = 0.\n",
    "        self.inital_decay = self.decay \n",
    "\n",
    "        dim =self.dim\n",
    "\n",
    "        self.m_W = [0.0 for _ in range(dim)]\n",
    "        self.v_W = [0.0 for _ in range(dim)]\n",
    "\n",
    "        self.m_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.v_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "\n",
    "        self.m_bias = 0.0\n",
    "        self.v_bias = 0.0\n",
    "\n",
    "\n",
    "    def adam_update(self,lr,x,residual):\n",
    "\n",
    "        if 0. < self.dropout < 1.:\n",
    "            self.droupout_x(x)\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.inital_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "\n",
    "        lr_t = lr * sqrt(1. - pow(self.beta_2, t)) / (1. - pow(self.beta_1, t))\n",
    "        \n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            g = residual*value\n",
    "\n",
    "            m = self.m_W[idx]\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "            v = self.v_W[idx]\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "            p = self.W[idx]\n",
    "            p_t = p - lr_t *m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            if self.l2>0:\n",
    "                p_t = p_t - lr_t*self.l2*p\n",
    "\n",
    "            self.m_W[idx] = m_t\n",
    "            self.v_W[idx] = v_t\n",
    "            self.W[idx] = p_t\n",
    "\n",
    "        if self.interaction:\n",
    "            self._adam_update_fm(lr_t,x,residual)\n",
    "\n",
    "\n",
    "        m = self.m_bias\n",
    "        m_t = (self.beta_1 * m) + (1. - self.beta_1)*residual\n",
    "\n",
    "        v = self.v_bias\n",
    "        v_t = (self.beta_2 * v) + (1. - self.beta_2)*(residual**2)\n",
    "\n",
    "        p = self.bias\n",
    "        p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "        if self.l2_bias > 0:\n",
    "            pt = pt - lr_t * self.l2_bias*p\n",
    "\n",
    "        self.m_bias = m_t\n",
    "        self.v_bias = v_t\n",
    "        self.bias = p_t\n",
    "\n",
    "        self.iterations+=1\n",
    "\n",
    "    def _adam_update_fm(self,lr_t,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                v = self.V[idx_i][f]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                g = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "\n",
    "                m = self.m_V[idx_i][f]\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "                v = self.v_V[idx_i][f]\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "                p = self.V[idx_i][f]\n",
    "                p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                if self.l2_fm>0:\n",
    "                    p_t = p_t - lr_t * self.l2_fm*p\n",
    "\n",
    "                self.m_V[idx_i][f] = m_t\n",
    "                self.v_V[idx_i][f] = v_t\n",
    "                self.V[idx_i][f] = p_t\n",
    "\n",
    "    def droupout_x(self, x):\n",
    "        new_x = []\n",
    "        for i, var in enumerate(x):\n",
    "            if random() > self.dropout:\n",
    "                del x[i]\n",
    "\n",
    "    def _predict_fm(self, x):\n",
    "        len_x = len(x)\n",
    "        n_components = self.n_components\n",
    "        pred = 0.0\n",
    "        self.sum_f_dict = {}\n",
    "        for f in range(n_components):\n",
    "            sum_f = 0.0\n",
    "            sum_sqr_f = 0.0\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                d = self.V[idx_i][f] * value_i\n",
    "                sum_f +=d\n",
    "                sum_sqr_f +=d*d\n",
    "            pred+= 0.5 * (sum_f*sum_f - sum_sqr_f);\n",
    "            self.sum_f_dict[f] = sum_f\n",
    "        return pred\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        pred = self.bias\n",
    "        # pred = 0.0\n",
    "        for idx,value in x:\n",
    "            pred+=self.W[idx]*value\n",
    "        \n",
    "        if self.interaction:\n",
    "            pred+=self._predict_fm(x)\n",
    "\n",
    "        if self.task=='c':\n",
    "            pred = bounded_sigmoid(pred)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _update_fm(self, lr, x, residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                v = self.V[idx_i][f]\n",
    "                grad = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "                \n",
    "                self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                if self.nesterov:\n",
    "                    self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                self.V[idx_i][f] = self.V[idx_i][f] + self.Velocity_V[idx_i][f] - lr*self.l2_fm*self.V[idx_i][f]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, lr, x, residual):\n",
    "\n",
    "        if 0. < self.dropout < 1.:\n",
    "            self.droupout_x(x)\n",
    "\n",
    "        for sample in x:\n",
    "            idx, value = sample\n",
    "            grad = residual * value\n",
    "            self.Velocity_W[idx] =  self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            if self.nesterov:\n",
    "                 self.Velocity_W[idx] = self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            self.W[idx] = self.W[idx] + self.Velocity_W[idx] - lr * self.l2 * self.W[idx]\n",
    "            \n",
    "        if self.interaction:\n",
    "            self._update_fm(lr, x, residual)\n",
    "\n",
    "        self.Velocity_bias = self.momentum * self.Velocity_bias - lr * residual\n",
    "        if self.nesterov:\n",
    "            self.Velocity_bias = self.momentum * self.Velocity_bias - lr * residual\n",
    "        self.bias = self.bias + self.Velocity_bias - lr * self.l2_bias * self.bias\n",
    "\n",
    "    def predict(self, path, out):\n",
    "\n",
    "        data = data_generator(path, self.no_norm,self.task)\n",
    "        y_preds =[]\n",
    "        with open(out, 'w') as outfile:\n",
    "            ID = 0\n",
    "            outfile.write('%s,%s\\n' % ('test_id', 'is_duplicate'))\n",
    "            for d in data:\n",
    "                x, y = d\n",
    "                p = self._predict_one(x)\n",
    "                outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "                ID+=1\n",
    "\n",
    "\n",
    "    def validate(self, path):\n",
    "        data = data_generator(path, self.no_norm, self.task)\n",
    "        loss = 0.0\n",
    "        count = 0.0\n",
    "\n",
    "        for d in data:\n",
    "            x,y = d\n",
    "            p = self._predict_one(x)\n",
    "            loss+=self.loss_function(y,p)\n",
    "            count+=1\n",
    "        return loss/count\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = []\n",
    "        weights.append(self.W)\n",
    "        weights.append(self.V)\n",
    "        weights.append(self.bias)\n",
    "        # weights.append(self.Velocity_W)\n",
    "        # weights.append(self.Velocity_V)\n",
    "        weights.append(self.dim)\n",
    "        pickle.dump(weights, open('sgd_fm.pkl','wb'))\n",
    "\n",
    "    def load_weights(self):\n",
    "        weights = pickle.load(open('sgd_fm.pkl','rb'))\n",
    "        self.W = weights[0]\n",
    "        self.V = weights[1]\n",
    "        self.bias = weights[2]\n",
    "        # self.Velocity_W = weights[3]\n",
    "        # self.Velocity_V = weights[4]\n",
    "        self.dim = weights[3]\n",
    "        \n",
    "\n",
    "    def train(self, path, valid_path=None, in_memory=False):\n",
    "\n",
    "        start = datetime.now()\n",
    "        lr = self.lr\n",
    "        if self.adam:\n",
    "            self.adam_init()\n",
    "            self.update = self.adam_update\n",
    "\n",
    "        if in_memory:\n",
    "            data = data_generator(path, self.no_norm, self.task) # Unfold compressed data into matrix.\n",
    "            data = [d for d in data]\n",
    "            \n",
    "        best_loss = 999999\n",
    "        best_epoch = 0\n",
    "        early_stop_count = 0\n",
    "        for epoch in range(1, self.nb_epoch+1): # Training through the epochs.\n",
    "            if not in_memory:\n",
    "                data = data_generator(path, self.no_norm, self.task)\n",
    "            train_loss = 0.0\n",
    "            train_count = 0\n",
    "            for x, y in data: # Training through the whole batch.\n",
    "                p = self._predict_one(x)\n",
    "                if self.task != 'c':                    \n",
    "                    residual = -(y-p)\n",
    "                else:\n",
    "                    # residual = -y*(1.0-1.0/(1.0+exp(-y*p)));\n",
    "                    residual = -(y-p)\n",
    "\n",
    "                self.update(lr, x, residual)\n",
    "                if train_count % 50000 == 0:\n",
    "                    if train_count == 0:\n",
    "                        print('\\ttrain_count: %s, current loss: %.6f'%(train_count, 0.0))\n",
    "                    else:\n",
    "                        print('\\ttrain_count: %s, current loss: %.6f'%(train_count, train_loss/train_count))\n",
    "\n",
    "                train_loss += self.loss_function(y,p)\n",
    "                train_count += 1\n",
    "\n",
    "            epoch_end = datetime.now()\n",
    "            duration = epoch_end - start\n",
    "            \n",
    "            # Early Stopping: save weights of the best epoch.\n",
    "            if valid_path:\n",
    "                valid_loss = self.validate(valid_path)\n",
    "                print('Epoch: %s, train loss: %.6f, valid loss: %.6f, time: %s'%(epoch, train_loss/train_count,valid_loss, duration))\n",
    "                print('early_stop_count: ', early_stop_count)\n",
    "                if valid_loss < best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    self.save_weights()\n",
    "                    early_stop_count = 0                    \n",
    "                    print('save_weights')\n",
    "                else:\n",
    "                    early_stop_count = early_stop_count + 1\n",
    "\n",
    "            else:\n",
    "                print('Epoch: %s, train loss: %.6f, time: %s'%(epoch,train_loss/train_count,duration))\n",
    "            \n",
    "            if early_stop_count >= 3:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"../kaggle-quora/data/\"\n",
    "\n",
    "sgd = SGD(lr=0.0001, \\\n",
    "          adam=True, \\\n",
    "          dropout=0.8, \\\n",
    "          l2=0.00, \\\n",
    "          l2_fm=0.00, \\\n",
    "          task='c', \\\n",
    "          n_components=1, \\\n",
    "          nb_epoch=30, \\\n",
    "          interaction=True, \\\n",
    "          no_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_2fold_stack_fm():\n",
    "    sgd.preload(path + 'X_fold1_tfidf_jacad_magic.svm', path + 'X_valid_tfidf_jacad_magic.svm')\n",
    "    sgd.train(path + 'X_fold1_tfidf_jacad_magic.svm', path + 'X_valid_tfidf_jacad_magic.svm', in_memory=False)\n",
    "    sgd.load_weights()\n",
    "    new_fold2 = sgd.predict(path + 'X_fold2_tfidf_jacad_magic.svm', out='fold2_jacad_magic.csv')\n",
    "    v1 = sgd.predict(path + 'X_valid_tfidf_jacad_magic.svm', out='v1_jacad_magic.csv')\n",
    "    \n",
    "    sgd.preload(path + 'X_fold2_tfidf_jacad_magic.svm', path + 'X_valid_tfidf_jacad_magic.svm')\n",
    "    sgd.train(path + 'X_fold2_tfidf_jacad_magic.svm', path + 'X_valid_tfidf_jacad_magic.svm', in_memory=False)\n",
    "    sgd.load_weights()\n",
    "    new_fold2 = sgd.predict(path + 'X_fold1_tfidf_jacad_magic.svm', out='fold1_jacad_magic.csv')\n",
    "    v1 = sgd.predict(path + 'X_valid_tfidf_jacad_magic.svm', out='v2_jacad_magic.csv')\n",
    "    \n",
    "    return np.concatenate([new_fold1, new_fold2], axis=0), (v1+v2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 260162\n",
      "Testing samples: 260161\n",
      "Number of features: 3073589\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.130314\n",
      "\ttrain_count: 100000, current loss: 1.707348\n",
      "\ttrain_count: 150000, current loss: 1.141425\n",
      "\ttrain_count: 200000, current loss: 0.856072\n",
      "\ttrain_count: 250000, current loss: 0.684858\n",
      "Epoch: 1, train loss: 0.658107, valid loss: 3.612401, time: 0:02:03.249870\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 10.380554\n",
      "\ttrain_count: 100000, current loss: 5.239365\n",
      "\ttrain_count: 150000, current loss: 3.492919\n",
      "\ttrain_count: 200000, current loss: 2.619689\n",
      "\ttrain_count: 250000, current loss: 2.095751\n",
      "Epoch: 2, train loss: 2.013891, valid loss: 3.879359, time: 0:04:43.163207\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 9.338040\n",
      "\ttrain_count: 100000, current loss: 4.771942\n",
      "\ttrain_count: 150000, current loss: 3.181356\n",
      "\ttrain_count: 200000, current loss: 2.386017\n",
      "\ttrain_count: 250000, current loss: 1.908814\n",
      "Epoch: 3, train loss: 1.834255, valid loss: 3.855387, time: 0:07:23.507434\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 7.988487\n",
      "\ttrain_count: 100000, current loss: 4.156953\n",
      "\ttrain_count: 150000, current loss: 2.771555\n",
      "\ttrain_count: 200000, current loss: 2.078668\n",
      "\ttrain_count: 250000, current loss: 1.662934\n",
      "Epoch: 4, train loss: 1.597979, valid loss: 3.774084, time: 0:10:00.492216\n",
      "early_stop_count:  2\n"
     ]
    }
   ],
   "source": [
    "sgd.preload(path + 'X_fold2_tfidf_jacad_magic.svm', path + 'X_valid_tfidf_jacad_magic.svm')\n",
    "sgd.train(path + 'X_fold2_tfidf_jacad_magic.svm', path + 'X_valid_tfidf_jacad_magic.svm', in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 260163\n",
      "Testing samples: 260161\n",
      "Number of features: 3073589\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.107588\n",
      "\ttrain_count: 100000, current loss: 0.321209\n",
      "\ttrain_count: 150000, current loss: 0.214139\n",
      "\ttrain_count: 200000, current loss: 0.160604\n",
      "\ttrain_count: 250000, current loss: 0.128483\n",
      "Epoch: 1, train loss: 0.123464, valid loss: 4.489009, time: 0:01:51.705392\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.784876\n",
      "\ttrain_count: 100000, current loss: 0.486043\n",
      "\ttrain_count: 150000, current loss: 0.324028\n",
      "\ttrain_count: 200000, current loss: 0.243021\n",
      "\ttrain_count: 250000, current loss: 0.194417\n",
      "Epoch: 2, train loss: 0.186822, valid loss: 4.546589, time: 0:04:31.546547\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.582862\n",
      "\ttrain_count: 100000, current loss: 0.391631\n",
      "\ttrain_count: 150000, current loss: 0.261087\n",
      "\ttrain_count: 200000, current loss: 0.195815\n",
      "\ttrain_count: 250000, current loss: 0.156652\n",
      "Epoch: 3, train loss: 0.150533, valid loss: 4.667871, time: 0:07:10.326108\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.544779\n",
      "\ttrain_count: 100000, current loss: 0.380326\n",
      "\ttrain_count: 150000, current loss: 0.253550\n",
      "\ttrain_count: 200000, current loss: 0.190163\n",
      "\ttrain_count: 250000, current loss: 0.152130\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-e41162ad1d5e>\u001b[0m in \u001b[0;36mget_2fold_stack_fm\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_2fold_stack_fm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_fold1_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_valid_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_fold1_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_valid_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_fold2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_fold2_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fold2_jacad_magic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9635e047ac63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, path, valid_path, in_memory)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Early Stopping: save weights of the best epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: %s, train loss: %.6f, valid loss: %.6f, time: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'early_stop_count: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9635e047ac63>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9635e047ac63>\u001b[0m in \u001b[0;36m_predict_one\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# pred = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_2fold_stack_fm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 404290\n",
      "Testing samples: 2345796\n",
      "Number of features: 3073589\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 1.464780\n",
      "\ttrain_count: 100000, current loss: 0.999318\n",
      "\ttrain_count: 150000, current loss: 0.814652\n",
      "\ttrain_count: 200000, current loss: 0.713237\n",
      "\ttrain_count: 250000, current loss: 0.647236\n",
      "\ttrain_count: 300000, current loss: 0.599801\n",
      "\ttrain_count: 350000, current loss: 0.564946\n",
      "\ttrain_count: 400000, current loss: 0.538342\n",
      "\ttrain_count: 450000, current loss: 0.516806\n",
      "\ttrain_count: 500000, current loss: 0.499143\n",
      "\ttrain_count: 550000, current loss: 0.484292\n",
      "\ttrain_count: 600000, current loss: 0.471913\n",
      "Epoch: 1, train loss: 0.466551, valid loss: 0.334514, time: 0:04:30.157538\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.327458\n",
      "\ttrain_count: 100000, current loss: 0.325485\n",
      "\ttrain_count: 150000, current loss: 0.325868\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-73768be069b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_t_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_train_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_test_tfidf_jacad_magic.svm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# sgd.load_weights()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9635e047ac63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, path, valid_path, in_memory)\u001b[0m\n\u001b[1;32m    369\u001b[0m                     \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrain_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9635e047ac63>\u001b[0m in \u001b[0;36madam_update\u001b[0;34m(self, lr, x, residual)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adam_update_fm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-9635e047ac63>\u001b[0m in \u001b[0;36m_adam_update_fm\u001b[0;34m(self, lr_t, x, residual)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_V\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mv_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### TODO: Delete ###\n",
    "\n",
    "sgd.preload(path + 'X_tfidf_jacad_magic.svm', path + 'X_t_tfidf_jacad_magic.svm')\n",
    "sgd.train(path + 'X_train_tfidf_jacad_magic.svm', path + 'X_test_tfidf_jacad_magic.svm', in_memory=False)\n",
    "\n",
    "sgd.preload(path + 'X_tfidf_jacad_magic.svm', path + 'X_t_tfidf_jacad_magic.svm')\n",
    "sgd.train(path + 'X_train_tfidf_jacad_magic.svm', path + 'X_test_tfidf_jacad_magic.svm', in_memory=False)\n",
    "\n",
    "# sgd.load_weights()\n",
    "# sgd.predict(path+'X_test_tfidf_jacad_magic.svm', out='valid_jacad_magic.csv')\n",
    "# print(sgd.validate(path+'X_test_tfidf_jacad_magic.svm'))\n",
    "# sgd.predict(path+'X_t_tfidf_jacad_magic.svm', out='pred_jacad_magic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: X_train: (780486, 50), Y_train: 780486, X_test: (2345796, 50)\n",
      "Will train XGB for 315 rounds, RandomSeed: 12357\n",
      "[0]\ttrain-logloss:0.629423\teval-logloss:0.629526\n",
      "[1]\ttrain-logloss:0.578195\teval-logloss:0.578343\n",
      "[2]\ttrain-logloss:0.536378\teval-logloss:0.53656\n",
      "[3]\ttrain-logloss:0.501217\teval-logloss:0.501432\n",
      "[4]\ttrain-logloss:0.4719\teval-logloss:0.472147\n",
      "[5]\ttrain-logloss:0.446716\teval-logloss:0.446973\n",
      "[6]\ttrain-logloss:0.425318\teval-logloss:0.425612\n",
      "[7]\ttrain-logloss:0.40699\teval-logloss:0.407314\n",
      "[8]\ttrain-logloss:0.391354\teval-logloss:0.391734\n",
      "[9]\ttrain-logloss:0.376584\teval-logloss:0.37698\n",
      "[10]\ttrain-logloss:0.364227\teval-logloss:0.364643\n",
      "[11]\ttrain-logloss:0.354132\teval-logloss:0.354596\n",
      "[12]\ttrain-logloss:0.344738\teval-logloss:0.345218\n",
      "[13]\ttrain-logloss:0.336595\teval-logloss:0.337109\n",
      "[14]\ttrain-logloss:0.328956\teval-logloss:0.329458\n",
      "[15]\ttrain-logloss:0.322348\teval-logloss:0.322837\n",
      "[16]\ttrain-logloss:0.316893\teval-logloss:0.317378\n",
      "[17]\ttrain-logloss:0.311979\teval-logloss:0.312502\n",
      "[18]\ttrain-logloss:0.307866\teval-logloss:0.308404\n",
      "[19]\ttrain-logloss:0.303598\teval-logloss:0.304162\n",
      "[20]\ttrain-logloss:0.299796\teval-logloss:0.300406\n",
      "[21]\ttrain-logloss:0.296428\teval-logloss:0.297058\n",
      "[22]\ttrain-logloss:0.29386\teval-logloss:0.294523\n",
      "[23]\ttrain-logloss:0.291221\teval-logloss:0.291934\n",
      "[24]\ttrain-logloss:0.289225\teval-logloss:0.289993\n",
      "[25]\ttrain-logloss:0.287198\teval-logloss:0.287988\n",
      "[26]\ttrain-logloss:0.285595\teval-logloss:0.28641\n",
      "[27]\ttrain-logloss:0.283868\teval-logloss:0.284683\n",
      "[28]\ttrain-logloss:0.282484\teval-logloss:0.283343\n",
      "[29]\ttrain-logloss:0.281381\teval-logloss:0.282247\n",
      "[30]\ttrain-logloss:0.280166\teval-logloss:0.281054\n",
      "[31]\ttrain-logloss:0.279082\teval-logloss:0.279951\n",
      "[32]\ttrain-logloss:0.277959\teval-logloss:0.278866\n",
      "[33]\ttrain-logloss:0.276992\teval-logloss:0.277891\n",
      "[34]\ttrain-logloss:0.276288\teval-logloss:0.277188\n",
      "[35]\ttrain-logloss:0.275447\teval-logloss:0.276373\n",
      "[36]\ttrain-logloss:0.27481\teval-logloss:0.275761\n",
      "[37]\ttrain-logloss:0.274113\teval-logloss:0.275108\n",
      "[38]\ttrain-logloss:0.273441\teval-logloss:0.274473\n",
      "[39]\ttrain-logloss:0.272679\teval-logloss:0.27371\n",
      "[40]\ttrain-logloss:0.272043\teval-logloss:0.273079\n",
      "[41]\ttrain-logloss:0.271617\teval-logloss:0.272667\n",
      "[42]\ttrain-logloss:0.271219\teval-logloss:0.272308\n",
      "[43]\ttrain-logloss:0.270718\teval-logloss:0.271862\n",
      "[44]\ttrain-logloss:0.2702\teval-logloss:0.271352\n",
      "[45]\ttrain-logloss:0.269853\teval-logloss:0.271021\n",
      "[46]\ttrain-logloss:0.269519\teval-logloss:0.270712\n",
      "[47]\ttrain-logloss:0.269197\teval-logloss:0.27043\n",
      "[48]\ttrain-logloss:0.268957\teval-logloss:0.270232\n",
      "[49]\ttrain-logloss:0.268555\teval-logloss:0.269841\n",
      "[50]\ttrain-logloss:0.268355\teval-logloss:0.269645\n",
      "[51]\ttrain-logloss:0.268095\teval-logloss:0.26943\n",
      "[52]\ttrain-logloss:0.267915\teval-logloss:0.269253\n",
      "[53]\ttrain-logloss:0.267635\teval-logloss:0.26899\n",
      "[54]\ttrain-logloss:0.267255\teval-logloss:0.268632\n",
      "[55]\ttrain-logloss:0.26705\teval-logloss:0.268448\n",
      "[56]\ttrain-logloss:0.266647\teval-logloss:0.268079\n",
      "[57]\ttrain-logloss:0.26646\teval-logloss:0.267925\n",
      "[58]\ttrain-logloss:0.266147\teval-logloss:0.26764\n",
      "[59]\ttrain-logloss:0.265994\teval-logloss:0.26752\n",
      "[60]\ttrain-logloss:0.265607\teval-logloss:0.26716\n",
      "[61]\ttrain-logloss:0.265442\teval-logloss:0.267016\n",
      "[62]\ttrain-logloss:0.265285\teval-logloss:0.266878\n",
      "[63]\ttrain-logloss:0.264836\teval-logloss:0.266458\n",
      "[64]\ttrain-logloss:0.26469\teval-logloss:0.266349\n",
      "[65]\ttrain-logloss:0.264401\teval-logloss:0.266085\n",
      "[66]\ttrain-logloss:0.264166\teval-logloss:0.265862\n",
      "[67]\ttrain-logloss:0.264007\teval-logloss:0.265737\n",
      "[68]\ttrain-logloss:0.263855\teval-logloss:0.265624\n",
      "[69]\ttrain-logloss:0.263597\teval-logloss:0.265355\n",
      "[70]\ttrain-logloss:0.263247\teval-logloss:0.265012\n",
      "[71]\ttrain-logloss:0.263005\teval-logloss:0.264802\n",
      "[72]\ttrain-logloss:0.262908\teval-logloss:0.264729\n",
      "[73]\ttrain-logloss:0.262684\teval-logloss:0.264529\n",
      "[74]\ttrain-logloss:0.26242\teval-logloss:0.264294\n",
      "[75]\ttrain-logloss:0.262062\teval-logloss:0.263949\n",
      "[76]\ttrain-logloss:0.261768\teval-logloss:0.263674\n",
      "[77]\ttrain-logloss:0.261638\teval-logloss:0.263569\n",
      "[78]\ttrain-logloss:0.261436\teval-logloss:0.263381\n",
      "[79]\ttrain-logloss:0.261336\teval-logloss:0.263298\n",
      "[80]\ttrain-logloss:0.261069\teval-logloss:0.263048\n",
      "[81]\ttrain-logloss:0.260919\teval-logloss:0.262916\n",
      "[82]\ttrain-logloss:0.260748\teval-logloss:0.262776\n",
      "[83]\ttrain-logloss:0.260645\teval-logloss:0.262678\n",
      "[84]\ttrain-logloss:0.260472\teval-logloss:0.262508\n",
      "[85]\ttrain-logloss:0.260281\teval-logloss:0.262348\n",
      "[86]\ttrain-logloss:0.260139\teval-logloss:0.262243\n",
      "[87]\ttrain-logloss:0.260024\teval-logloss:0.262156\n",
      "[88]\ttrain-logloss:0.259933\teval-logloss:0.26209\n",
      "[89]\ttrain-logloss:0.259817\teval-logloss:0.261993\n",
      "[90]\ttrain-logloss:0.259696\teval-logloss:0.261888\n",
      "[91]\ttrain-logloss:0.259588\teval-logloss:0.261792\n",
      "[92]\ttrain-logloss:0.259432\teval-logloss:0.26166\n",
      "[93]\ttrain-logloss:0.25925\teval-logloss:0.261515\n",
      "[94]\ttrain-logloss:0.259004\teval-logloss:0.261298\n",
      "[95]\ttrain-logloss:0.258901\teval-logloss:0.26123\n",
      "[96]\ttrain-logloss:0.258801\teval-logloss:0.261162\n",
      "[97]\ttrain-logloss:0.258563\teval-logloss:0.26096\n",
      "[98]\ttrain-logloss:0.25833\teval-logloss:0.260721\n",
      "[99]\ttrain-logloss:0.258277\teval-logloss:0.260676\n",
      "[100]\ttrain-logloss:0.258163\teval-logloss:0.260575\n",
      "[101]\ttrain-logloss:0.25804\teval-logloss:0.260492\n",
      "[102]\ttrain-logloss:0.257747\teval-logloss:0.26023\n",
      "[103]\ttrain-logloss:0.257652\teval-logloss:0.260151\n",
      "[104]\ttrain-logloss:0.257537\teval-logloss:0.260061\n",
      "[105]\ttrain-logloss:0.257276\teval-logloss:0.259817\n",
      "[106]\ttrain-logloss:0.257204\teval-logloss:0.259749\n",
      "[107]\ttrain-logloss:0.257081\teval-logloss:0.259636\n",
      "[108]\ttrain-logloss:0.256971\teval-logloss:0.259559\n",
      "[109]\ttrain-logloss:0.256878\teval-logloss:0.259495\n",
      "[110]\ttrain-logloss:0.256673\teval-logloss:0.259333\n",
      "[111]\ttrain-logloss:0.25652\teval-logloss:0.259203\n",
      "[112]\ttrain-logloss:0.256443\teval-logloss:0.259157\n",
      "[113]\ttrain-logloss:0.256374\teval-logloss:0.259121\n",
      "[114]\ttrain-logloss:0.256188\teval-logloss:0.258939\n",
      "[115]\ttrain-logloss:0.256081\teval-logloss:0.258841\n",
      "[116]\ttrain-logloss:0.255993\teval-logloss:0.258762\n",
      "[117]\ttrain-logloss:0.255743\teval-logloss:0.258549\n",
      "[118]\ttrain-logloss:0.25563\teval-logloss:0.258446\n",
      "[119]\ttrain-logloss:0.255591\teval-logloss:0.258421\n",
      "[120]\ttrain-logloss:0.25554\teval-logloss:0.258399\n",
      "[121]\ttrain-logloss:0.255471\teval-logloss:0.25835\n",
      "[122]\ttrain-logloss:0.255229\teval-logloss:0.258125\n",
      "[123]\ttrain-logloss:0.255139\teval-logloss:0.258059\n",
      "[124]\ttrain-logloss:0.255006\teval-logloss:0.257957\n",
      "[125]\ttrain-logloss:0.254933\teval-logloss:0.257918\n",
      "[126]\ttrain-logloss:0.254795\teval-logloss:0.257821\n",
      "[127]\ttrain-logloss:0.254712\teval-logloss:0.257744\n",
      "[128]\ttrain-logloss:0.254605\teval-logloss:0.25765\n",
      "[129]\ttrain-logloss:0.254555\teval-logloss:0.257608\n",
      "[130]\ttrain-logloss:0.254477\teval-logloss:0.257553\n",
      "[131]\ttrain-logloss:0.254396\teval-logloss:0.257481\n",
      "[132]\ttrain-logloss:0.254338\teval-logloss:0.257439\n",
      "[133]\ttrain-logloss:0.254131\teval-logloss:0.257242\n",
      "[134]\ttrain-logloss:0.253799\teval-logloss:0.256912\n",
      "[135]\ttrain-logloss:0.253709\teval-logloss:0.256849\n",
      "[136]\ttrain-logloss:0.253667\teval-logloss:0.25682\n",
      "[137]\ttrain-logloss:0.253506\teval-logloss:0.256674\n",
      "[138]\ttrain-logloss:0.25345\teval-logloss:0.256626\n",
      "[139]\ttrain-logloss:0.253318\teval-logloss:0.256523\n",
      "[140]\ttrain-logloss:0.253226\teval-logloss:0.256452\n",
      "[141]\ttrain-logloss:0.253155\teval-logloss:0.256399\n",
      "[142]\ttrain-logloss:0.253029\teval-logloss:0.256268\n",
      "[143]\ttrain-logloss:0.252972\teval-logloss:0.256224\n",
      "[144]\ttrain-logloss:0.252824\teval-logloss:0.256111\n",
      "[145]\ttrain-logloss:0.252669\teval-logloss:0.255967\n",
      "[146]\ttrain-logloss:0.252611\teval-logloss:0.255922\n",
      "[147]\ttrain-logloss:0.252452\teval-logloss:0.255803\n",
      "[148]\ttrain-logloss:0.252365\teval-logloss:0.255742\n",
      "[149]\ttrain-logloss:0.252121\teval-logloss:0.255499\n",
      "[150]\ttrain-logloss:0.252058\teval-logloss:0.255467\n",
      "[151]\ttrain-logloss:0.252035\teval-logloss:0.255448\n",
      "[152]\ttrain-logloss:0.251997\teval-logloss:0.255417\n",
      "[153]\ttrain-logloss:0.251915\teval-logloss:0.255337\n",
      "[154]\ttrain-logloss:0.251832\teval-logloss:0.255279\n",
      "[155]\ttrain-logloss:0.251723\teval-logloss:0.255187\n",
      "[156]\ttrain-logloss:0.251546\teval-logloss:0.255064\n",
      "[157]\ttrain-logloss:0.251377\teval-logloss:0.254929\n",
      "[158]\ttrain-logloss:0.251252\teval-logloss:0.254824\n",
      "[159]\ttrain-logloss:0.251154\teval-logloss:0.254755\n",
      "[160]\ttrain-logloss:0.251034\teval-logloss:0.25466\n",
      "[161]\ttrain-logloss:0.250943\teval-logloss:0.254591\n",
      "[162]\ttrain-logloss:0.250894\teval-logloss:0.254555\n",
      "[163]\ttrain-logloss:0.250799\teval-logloss:0.254484\n",
      "[164]\ttrain-logloss:0.250626\teval-logloss:0.254319\n",
      "[165]\ttrain-logloss:0.250502\teval-logloss:0.254225\n",
      "[166]\ttrain-logloss:0.250445\teval-logloss:0.254176\n",
      "[167]\ttrain-logloss:0.250415\teval-logloss:0.254164\n",
      "[168]\ttrain-logloss:0.250331\teval-logloss:0.254103\n",
      "[169]\ttrain-logloss:0.250144\teval-logloss:0.25392\n",
      "[170]\ttrain-logloss:0.250124\teval-logloss:0.253907\n",
      "[171]\ttrain-logloss:0.250045\teval-logloss:0.253855\n",
      "[172]\ttrain-logloss:0.249986\teval-logloss:0.253805\n",
      "[173]\ttrain-logloss:0.249942\teval-logloss:0.253779\n",
      "[174]\ttrain-logloss:0.249823\teval-logloss:0.253686\n",
      "[175]\ttrain-logloss:0.249614\teval-logloss:0.253478\n",
      "[176]\ttrain-logloss:0.249516\teval-logloss:0.253423\n",
      "[177]\ttrain-logloss:0.249465\teval-logloss:0.253406\n",
      "[178]\ttrain-logloss:0.249441\teval-logloss:0.253389\n",
      "[179]\ttrain-logloss:0.249306\teval-logloss:0.253281\n",
      "[180]\ttrain-logloss:0.249286\teval-logloss:0.253271\n",
      "[181]\ttrain-logloss:0.249263\teval-logloss:0.253257\n",
      "[182]\ttrain-logloss:0.249134\teval-logloss:0.253176\n",
      "[183]\ttrain-logloss:0.249082\teval-logloss:0.25314\n",
      "[184]\ttrain-logloss:0.248958\teval-logloss:0.253044\n",
      "[185]\ttrain-logloss:0.248931\teval-logloss:0.253021\n",
      "[186]\ttrain-logloss:0.24888\teval-logloss:0.253\n",
      "[187]\ttrain-logloss:0.248821\teval-logloss:0.252961\n",
      "[188]\ttrain-logloss:0.248708\teval-logloss:0.252867\n",
      "[189]\ttrain-logloss:0.248606\teval-logloss:0.252773\n",
      "[190]\ttrain-logloss:0.248596\teval-logloss:0.252766\n",
      "[191]\ttrain-logloss:0.248489\teval-logloss:0.252672\n",
      "[192]\ttrain-logloss:0.248426\teval-logloss:0.252635\n",
      "[193]\ttrain-logloss:0.248323\teval-logloss:0.252545\n",
      "[194]\ttrain-logloss:0.248193\teval-logloss:0.252438\n",
      "[195]\ttrain-logloss:0.248129\teval-logloss:0.252401\n",
      "[196]\ttrain-logloss:0.248044\teval-logloss:0.252339\n",
      "[197]\ttrain-logloss:0.247989\teval-logloss:0.252316\n",
      "[198]\ttrain-logloss:0.24796\teval-logloss:0.252304\n",
      "[199]\ttrain-logloss:0.247942\teval-logloss:0.252296\n",
      "[200]\ttrain-logloss:0.247896\teval-logloss:0.252254\n",
      "[201]\ttrain-logloss:0.24784\teval-logloss:0.252218\n",
      "[202]\ttrain-logloss:0.247828\teval-logloss:0.252217\n",
      "[203]\ttrain-logloss:0.247768\teval-logloss:0.252166\n",
      "[204]\ttrain-logloss:0.247607\teval-logloss:0.252015\n",
      "[205]\ttrain-logloss:0.247508\teval-logloss:0.251934\n",
      "[206]\ttrain-logloss:0.247448\teval-logloss:0.251906\n",
      "[207]\ttrain-logloss:0.247318\teval-logloss:0.251806\n",
      "[208]\ttrain-logloss:0.247215\teval-logloss:0.251725\n",
      "[209]\ttrain-logloss:0.247179\teval-logloss:0.251693\n",
      "[210]\ttrain-logloss:0.247103\teval-logloss:0.251658\n",
      "[211]\ttrain-logloss:0.247037\teval-logloss:0.251606\n",
      "[212]\ttrain-logloss:0.24677\teval-logloss:0.251341\n",
      "[213]\ttrain-logloss:0.246745\teval-logloss:0.251318\n",
      "[214]\ttrain-logloss:0.246698\teval-logloss:0.25129\n",
      "[215]\ttrain-logloss:0.246658\teval-logloss:0.251265\n",
      "[216]\ttrain-logloss:0.24664\teval-logloss:0.251255\n",
      "[217]\ttrain-logloss:0.246553\teval-logloss:0.251178\n",
      "[218]\ttrain-logloss:0.246454\teval-logloss:0.251115\n",
      "[219]\ttrain-logloss:0.246384\teval-logloss:0.251067\n",
      "[220]\ttrain-logloss:0.246324\teval-logloss:0.251025\n",
      "[221]\ttrain-logloss:0.246213\teval-logloss:0.250931\n",
      "[222]\ttrain-logloss:0.246102\teval-logloss:0.250845\n",
      "[223]\ttrain-logloss:0.246051\teval-logloss:0.250805\n",
      "[224]\ttrain-logloss:0.245948\teval-logloss:0.25073\n",
      "[225]\ttrain-logloss:0.245897\teval-logloss:0.250699\n",
      "[226]\ttrain-logloss:0.245849\teval-logloss:0.250671\n",
      "[227]\ttrain-logloss:0.245739\teval-logloss:0.250598\n",
      "[228]\ttrain-logloss:0.245686\teval-logloss:0.250571\n",
      "[229]\ttrain-logloss:0.245624\teval-logloss:0.250523\n",
      "[230]\ttrain-logloss:0.245536\teval-logloss:0.250461\n",
      "[231]\ttrain-logloss:0.245497\teval-logloss:0.250431\n",
      "[232]\ttrain-logloss:0.245449\teval-logloss:0.250387\n",
      "[233]\ttrain-logloss:0.245441\teval-logloss:0.250384\n",
      "[234]\ttrain-logloss:0.245415\teval-logloss:0.250375\n",
      "[235]\ttrain-logloss:0.245364\teval-logloss:0.250347\n",
      "[236]\ttrain-logloss:0.245334\teval-logloss:0.250333\n",
      "[237]\ttrain-logloss:0.245247\teval-logloss:0.250238\n",
      "[238]\ttrain-logloss:0.245084\teval-logloss:0.250096\n",
      "[239]\ttrain-logloss:0.245061\teval-logloss:0.250078\n",
      "[240]\ttrain-logloss:0.245032\teval-logloss:0.250068\n",
      "[241]\ttrain-logloss:0.245019\teval-logloss:0.250063\n",
      "[242]\ttrain-logloss:0.245013\teval-logloss:0.250061\n",
      "[243]\ttrain-logloss:0.244892\teval-logloss:0.24997\n",
      "[244]\ttrain-logloss:0.244852\teval-logloss:0.249946\n",
      "[245]\ttrain-logloss:0.244802\teval-logloss:0.249918\n",
      "[246]\ttrain-logloss:0.244687\teval-logloss:0.249829\n",
      "[247]\ttrain-logloss:0.244644\teval-logloss:0.249814\n",
      "[248]\ttrain-logloss:0.244618\teval-logloss:0.249801\n",
      "[249]\ttrain-logloss:0.244574\teval-logloss:0.249771\n",
      "[250]\ttrain-logloss:0.244469\teval-logloss:0.249666\n",
      "[251]\ttrain-logloss:0.244452\teval-logloss:0.249663\n",
      "[252]\ttrain-logloss:0.244397\teval-logloss:0.249628\n",
      "[253]\ttrain-logloss:0.244339\teval-logloss:0.249595\n",
      "[254]\ttrain-logloss:0.244265\teval-logloss:0.249559\n",
      "[255]\ttrain-logloss:0.244185\teval-logloss:0.24951\n",
      "[256]\ttrain-logloss:0.244139\teval-logloss:0.249475\n",
      "[257]\ttrain-logloss:0.244046\teval-logloss:0.249423\n",
      "[258]\ttrain-logloss:0.243973\teval-logloss:0.249362\n",
      "[259]\ttrain-logloss:0.243919\teval-logloss:0.249327\n",
      "[260]\ttrain-logloss:0.243898\teval-logloss:0.249319\n",
      "[261]\ttrain-logloss:0.243881\teval-logloss:0.249316\n",
      "[262]\ttrain-logloss:0.243874\teval-logloss:0.249313\n",
      "[263]\ttrain-logloss:0.243798\teval-logloss:0.249262\n",
      "[264]\ttrain-logloss:0.243723\teval-logloss:0.249218\n",
      "[265]\ttrain-logloss:0.243719\teval-logloss:0.249216\n",
      "[266]\ttrain-logloss:0.243709\teval-logloss:0.249212\n",
      "[267]\ttrain-logloss:0.243643\teval-logloss:0.249158\n",
      "[268]\ttrain-logloss:0.24356\teval-logloss:0.249095\n",
      "[269]\ttrain-logloss:0.243534\teval-logloss:0.249085\n",
      "[270]\ttrain-logloss:0.243467\teval-logloss:0.249036\n",
      "[271]\ttrain-logloss:0.243422\teval-logloss:0.24902\n",
      "[272]\ttrain-logloss:0.243387\teval-logloss:0.248998\n",
      "[273]\ttrain-logloss:0.243356\teval-logloss:0.248978\n",
      "[274]\ttrain-logloss:0.243337\teval-logloss:0.248969\n",
      "[275]\ttrain-logloss:0.243262\teval-logloss:0.248922\n",
      "[276]\ttrain-logloss:0.243193\teval-logloss:0.248869\n",
      "[277]\ttrain-logloss:0.243139\teval-logloss:0.248837\n",
      "[278]\ttrain-logloss:0.24307\teval-logloss:0.248796\n",
      "[279]\ttrain-logloss:0.243028\teval-logloss:0.248764\n",
      "[280]\ttrain-logloss:0.242979\teval-logloss:0.248731\n",
      "[281]\ttrain-logloss:0.242922\teval-logloss:0.248687\n",
      "[282]\ttrain-logloss:0.242894\teval-logloss:0.248664\n",
      "[283]\ttrain-logloss:0.242829\teval-logloss:0.248614\n",
      "[284]\ttrain-logloss:0.242724\teval-logloss:0.24853\n",
      "[285]\ttrain-logloss:0.242686\teval-logloss:0.248509\n",
      "[286]\ttrain-logloss:0.242681\teval-logloss:0.248506\n",
      "[287]\ttrain-logloss:0.242675\teval-logloss:0.248503\n",
      "[288]\ttrain-logloss:0.242667\teval-logloss:0.248503\n",
      "[289]\ttrain-logloss:0.242576\teval-logloss:0.24844\n",
      "[290]\ttrain-logloss:0.242514\teval-logloss:0.248398\n",
      "[291]\ttrain-logloss:0.242479\teval-logloss:0.24838\n",
      "[292]\ttrain-logloss:0.242472\teval-logloss:0.248377\n",
      "[293]\ttrain-logloss:0.242447\teval-logloss:0.248367\n",
      "[294]\ttrain-logloss:0.242391\teval-logloss:0.248351\n",
      "[295]\ttrain-logloss:0.242357\teval-logloss:0.24833\n",
      "[296]\ttrain-logloss:0.242342\teval-logloss:0.248325\n",
      "[297]\ttrain-logloss:0.242314\teval-logloss:0.248302\n",
      "[298]\ttrain-logloss:0.242259\teval-logloss:0.248271\n",
      "[299]\ttrain-logloss:0.242194\teval-logloss:0.248224\n",
      "[300]\ttrain-logloss:0.242114\teval-logloss:0.248167\n",
      "[301]\ttrain-logloss:0.242043\teval-logloss:0.248125\n",
      "[302]\ttrain-logloss:0.242003\teval-logloss:0.248096\n",
      "[303]\ttrain-logloss:0.241981\teval-logloss:0.248082\n",
      "[304]\ttrain-logloss:0.241954\teval-logloss:0.248069\n",
      "[305]\ttrain-logloss:0.241949\teval-logloss:0.248063\n",
      "[306]\ttrain-logloss:0.241876\teval-logloss:0.247997\n",
      "[307]\ttrain-logloss:0.241865\teval-logloss:0.24799\n",
      "[308]\ttrain-logloss:0.241862\teval-logloss:0.247991\n",
      "[309]\ttrain-logloss:0.241857\teval-logloss:0.247986\n",
      "[310]\ttrain-logloss:0.241757\teval-logloss:0.247907\n",
      "[311]\ttrain-logloss:0.241664\teval-logloss:0.247849\n",
      "[312]\ttrain-logloss:0.241618\teval-logloss:0.247819\n",
      "[313]\ttrain-logloss:0.24151\teval-logloss:0.247736\n",
      "[314]\ttrain-logloss:0.241484\teval-logloss:0.247724\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\n",
    "clr = train_xgb(x_train, y_train, params)\n",
    "preds = predict_xgb(clr, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output...\n",
      "Features importances...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAVxCAYAAACgGcU2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4wxxgws/wJZZ/s7p1TlqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11240e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Writing output...\")\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = preds *.75\n",
    "sub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n",
    "\n",
    "print(\"Features importances...\")\n",
    "importance = clr.get_fscore(fmap='xgb.fmap')\n",
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "ft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "\n",
    "ft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n",
    "plt.gcf().savefig('features_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
